

```{r all_data, message=FALSE, warning=FALSE, include=FALSE}
# Load raw data 
# Filter SP verification responses
SP_V <-  dir(path = "..",
      pattern = "all_rawdata_SP_V",   ## include in-site and internet data
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
      subset(Match != "F") %>% #only exclude fillers so we can check accuracy 
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id

Raw_total <- SP_V %>%
    group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
    group_by(Language, PSA_ID) %>%
    summarise(Raw_N = n())

# Load PP verification responses
PP <- dir(path = "..",
      pattern = "all_rawdata_PP", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
      subset(Identical != "F") %>% #only exclude filler so we can check accuracy
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id


# Load SP memory responses
SP_M <- dir(path = "..",
      pattern = "all_rawdata_SP_M", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv()  %>%    
      # subset(correct == 1) %>%  ## Exclude the incorrect responses and filler trials
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id
```

```{r erin_counts, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# check for participants with too many trials
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())


## site_CAN_020_1
## View(SP_V %>% filter(Subject == "site_CAN_020_1"))
SP_V$Subject[SP_V$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 24),
                                                    rep("site_CAN_020_1_2", 24))
## osweb_PSA_002_587
## View(SP_V %>% filter(Subject == "osweb_PSA_002_587"))
## nrow(SP_V)
SP_V <- SP_V %>% 
  group_by(Subject) %>% 
  filter(!duplicated(Target))
## nrow(SP_V)
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())

PP_counts <- PP %>% group_by(Subject) %>% summarize(n = n())

SP_M_counts <- SP_M %>% group_by(Subject) %>% summarize(n = n())

SP_M$Subject[SP_M$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 11), 
                                                    rep("site_CAN_020_1_2", 11))



SP_V_counts  %>% filter(n!=24)

PP_counts %>% filter(n!=24)  ## post-study survey had no labels in OSWEB script
```

```{r site_SP_V, message=FALSE, warning=FALSE, include=FALSE}
## Exclude the participants who had a accuracy lower than the preregistered exclusion criterion (70%) according to the previous study (Chen et al., 2020)

## Summarize the valid participants' SP verification data
SP_V_subj_site <- SP_V %>%
    filter(opensesame_codename!="osweb") %>%  # exclude jatos data
    group_by(Subject) %>%
    mutate(acc = sum(correct)/n()) %>%
    #mutate(acc = n()/24) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
##    summarise(V_RT = median(response_time), V_Acc = n()/12)
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n())  ## Erin revised the code code because the wrong responses did not

## Export for app3
write_csv(SP_V_subj_site, file="SP_V_subj_site.csv")

## Tidy SP V data for mixed linear model
SP_V_site_tidy <- SP_V %>% 
  filter(Source!="osweb")
```


```{r site_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_site_tidy <- SP_M %>% 
    filter(Source != "osweb")

## Summarize the valid participants' SP memory data
SP_M_subj_site <- SP_M_site_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
#  summarise(M_Acc = n()/11)
  summarise(M_Acc = sum(correct)/n())
```



```{r site_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_site_tidy <- PP %>% 
    filter(Source!="osweb") 


## Summarize the valid participants' PP verification data
PP_subj_site <- PP_site_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(P_RT = median(response_time), P_Acc = n()/12) 
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```


```{r count_site, message=FALSE, warning=FALSE, include=FALSE}
sum_site <- (SP_V_site_tidy %>% # first part: SP V
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
    group_by(Language, PSA_ID) %>%
  summarise(SP_N = n())) %>% #divide by 2 for match/no match 
left_join(
  SP_M_subj_site %>%           # second part: memory check
    group_by(Language, PSA_ID) %>%
    summarise(M_N = n()),
  by = c("Language","PSA_ID")) %>%
left_join(  
(PP_site_tidy %>%     # third part: PP
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n())),   #divide by 2 for identical/different orientations 
by=c("Language","PSA_ID")
) 

##sum_site %>% filter((SP_N != M_N) & (SP_N != PP_N))
```


```{r online_SP_V, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP V data for mixed linear model
SP_V_osweb_tidy <-  SP_V %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
    subset(Match != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP



## Summarize the valid participants' SP verification data
SP_V_subj_osweb <- SP_V_osweb_tidy %>%
#    group_by(subject_nr) %>%
    group_by(Subject) %>%
#    mutate(acc = n()/24) %>%
    mutate(acc = sum(correct)/n()) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(V_RT = median(response_time), V_Acc = n()/12) 
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n()) 

## Export for app3
write_csv(SP_V_subj_osweb, file="SP_V_subj_osweb.csv")


```


```{r online_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_osweb_tidy <-  SP_M %>%
      filter(Source=="osweb") %>%   # include jatos data
      distinct() %>% ## Merge the language aspects
      filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' SP memory data
SP_M_subj_osweb <- SP_M_osweb_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
  #group_by(Language, PSA_ID, subject_nr) %>% 
  summarise(M_Acc = sum(correct)/n())
  #summarise(M_Acc = n()/11)
```


```{r online_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_osweb_tidy <-  PP %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
    subset(Identical != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' PP verification data
PP_subj_osweb <- PP_osweb_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(P_RT = median(response_time), P_Acc = n()/12) 
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```


```{r functions_pak02, message=FALSE, warning=FALSE, include=FALSE}
## Compute the participants from labs and from Internet
sum_osweb <- (SP_V_osweb_tidy %>%
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(SP_N = n())) %>%
left_join(  
(PP_osweb_tidy %>% 
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n())),
by=c("Language","PSA_ID")
) 

firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}
```

# Results


Within the data collected on-site, `r format(sum(sum_site$SP_N), scientific=FALSE, big.mark = ",")` participants finished the sentence-picture verification task and met the preregistered inclusion criterion (accuracy percentile > 70%); `r format(sum(sum_site$PP_N), scientific=FALSE, big.mark = ",")` participants finished the picture-picture verification task. Raw data files containing data for `r xfun::numbers_to_words(sum(sum_site$PP_N) - sum(sum_site$SP_N))` participants were lost due to human error. Within the data sets collected online, `r format(sum(sum_osweb$SP_N), scientific=FALSE, big.mark = ",")` participants finished the sentence-picture verification task and met the preregistered inclusion criterion; `r format(sum(sum_osweb$PP_N), scientific=FALSE, big.mark = ",")` participants finished the picture-picture verification task. <!--- All data and analyses are available on the source files (https://osf.io/p7avr/).  ---> `r sum(Raw_total$Raw_N) - sum(sum_site$SP_N) - sum(sum_osweb$SP_N)` participant was removed because they did not reach our accuracy criterion.


## Intra-lab analysis during data collection

Before data collection, each lab decided whether they wanted to apply a sequential analysis [@schonbrodtSequentialHypothesisTesting2017] or whether they wanted to settle for a fixed sample size. The preregistered protocol for labs applying sequential analysis established that they could stop data collection upon reaching the preregistered criterion ($BF_{10} = 10\ or\ -10$), or the maximal sample size. Each laboratory chose a fixed sample size and an incremental _n_ for sequential analysis before their data collection. <!--- Most laboratories either chose a fixed sample size without applying sequential analysis, or applied sequential analysis and reached their maximal sample size. ---> Two laboratories (HUN 001, TWN 001) stopped data collection at the preregistered criterion, $BF_{10} = -10$. <!---Some--->Fourteen laboratories did not conduct the sequential analysis on all their data because of one of the following reasons: (1) their data collection was interrupted by the pandemic outbreak; (2) participants performed worse in the online study; (3) two non-English laboratories (TUR_007, TWN_002) recruited English-speaking participants for the institutional policies<!---too many of their participants were non-native speakers--->. Lab-specific results were reported on a public website as each laboratory completed data collection (details available in Appendix 2).


## Inter-lab analysis of final data


```{r preparation, message=FALSE, warning=FALSE, include=FALSE}
if(sum(names(SP_V_site_tidy) == names(SP_V_osweb_tidy)) == dim(SP_V_site_tidy)[2]){
  SP_V_tidy = bind_rows(SP_V_site_tidy, SP_V_osweb_tidy)
    chunk_msg01 <- c("All columns in SP_V matched")
} else {
  chunk_msg01 <- c("Not all columns in SP_V matched")
}

if(sum(names(PP_site_tidy) == names(PP_osweb_tidy)) == dim(PP_site_tidy)[2]){
  PP_tidy = bind_rows(PP_site_tidy, PP_osweb_tidy)
    chunk_msg02 <- c("All columns in PP matched")
} else {
  chunk_msg02 <- c("Not all columns in PP matched")
}


if(sum(names(SP_M_site_tidy) == names(SP_M_osweb_tidy)) == dim(SP_M_site_tidy)[2]){
  SP_M_tidy = bind_rows(SP_M_site_tidy, SP_M_osweb_tidy)
    chunk_msg03 <- c("All columns in SP_M matched")
} else {
  chunk_msg03 <- c("Not all columns in SP_M matched")
}
```


```{r erin_exclude_incorrects, include = FALSE, echo = FALSE}
nrow(SP_V_tidy)
SP_V_tidy <- SP_V_tidy %>% 
  filter(correct == 1)
nrow(SP_V_tidy)

nrow(PP_tidy)
PP_tidy <- PP_tidy %>% 
  filter(correct == 1)
nrow(PP_tidy)

nrow(SP_M_tidy)
SP_M_tidy <- SP_M_tidy %>% 
  filter(correct == 1)
nrow(SP_M_tidy)
```

```{r erin_outliers}
# We will implement a minimum response latency 160
# We will use a 2*MAD criterion to eliminate long response latencies
# SP_V_tidy and PP_tidy has the variable "Outlier" denoted the outlier.
SP_V_tidy <- SP_V_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time),
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 


## Export for app3
write_csv(SP_V_tidy, file="SP_V_tidy.csv")


PP_tidy <- PP_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time), 
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 

SP_M_tidy <- SP_M_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time)) 

# Integrate this into the outlier analysis table, change out for lmer criterion and say why
```



```{r cycle_outlier_check, message=FALSE, warning=FALSE, include=FALSE}
get_intercept <- function(set) {
  t <- cbind(
             Subject = levels(as.factor(set$Subject)),
             (lmer(response_time ~ Match + (1|Subject), data = set) %>%
  coef())$Subject %>%
      as_tibble()%>%
  mutate(#LowerBound = quantile(`(Intercept)`,probs=.25),
         #UpperBound = quantile(`(Intercept)`,probs=.75),
         Mark = 
           (`(Intercept)` > quantile(`(Intercept)`,probs=.75)) | (`(Intercept)` < quantile(`(Intercept)`,probs=.25)) )
    ) ## We should exclude the unusual fastest responses
  
  return(t)
}

LABS <- unique(SP_V_tidy$PSA_ID)

## Marks outliers by lme

outliers_marks <- NULL
for(lab_id in LABS){
  outliers_marks<- get_intercept( subset(SP_V_tidy,PSA_ID == lab_id)) %>%
    mutate(LAB = lab_id) %>%
    rbind(outliers_marks)
}


## Erin's revised outliers table are by MAD. The codes after this chunk can not access this table

outliers_table <- SP_V_tidy %>% 
    group_by(PSA_ID) %>% 
    summarize(total_n = length(unique(Subject)), 
              total_data = n(), 
              total_outliers = sum(Outlier == T), 
              prop = round(total_outliers / total_data, 2))




## write_csv(outliers_table, file = "includes/files/outliers_table.csv")

## Summarize outliers by lab
## Show the table in Appendix (#3)
#outliers_dist<- outliers_table %>%
#  group_by(LAB) %>%
#  summarise(N = sum(Outlier), Prop = round(sum(Outlier)/n(),2) ) #%>%
#  rmarkdown::paged_table(options = list(rows.print = 10))
```


```{r summary-languages, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}

## Count the available participants
Prereg_N <- SP_V_tidy %>% 
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>% 
  filter(Mark == FALSE) %>% ## Outlier by intercept
  group_by(PSA_ID, Subject) %>%
  summarise(N = n()) %>%
  group_by(PSA_ID) %>%
  summarise(Lab_N = n()) #%>%
  #summarise(Total = sum(Lab_N))

MAD_N <- SP_V_tidy %>% 
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>% 
  filter(Outlier == FALSE) %>% ## Outlier by MAD
  group_by(PSA_ID, Subject) %>%
  summarise(N = n()) %>%
  group_by(PSA_ID) %>%
  summarise(Lab_N = n()) 

# length(unique(SP_V_tidy$Subject))

# note that V_RT is before outliers and percent correct, subject_M is after both 
SP_V_tidy %>% 
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>% 
  filter(Outlier == FALSE) %>% ## Outlier by MAD
#  filter(Mark == FALSE & Source == "site") %>% ## Outlier by Shrinkage
  group_by(Source, Language, Subject, Match) %>%
  summarise(subject_M = median(response_time), subject_ACC = n()/12) %>%
  group_by(Source, Language, Match) %>%
  summarise(N = n(), med_RT = median(subject_M), mean_ACC =   mean(subject_ACC)) %>%
  pivot_wider(names_from = Match, values_from = c(med_RT, mean_ACC)) %>%
  mutate(Effect = (med_RT_N - med_RT_Y) ) %>%
  transmute( N=N,
             Mismatch_stat = paste0(round(med_RT_N),"(",format(round(mean_ACC_N*100,digits=2),nsmall=2),")"),
             Match_stat = paste0(round(med_RT_Y),"(",format(round(mean_ACC_Y*100,digits=2),nsmall=2),")"),
             Effect = Effect) %>%
  mutate(Source = if_else(Source=="osweb","Internet","Site")) %>%
  arrange(desc(Source)) %>%
  kable(  
    format = "latex",
    booktabs = TRUE,
    escape=FALSE,
    col.names = c("Study Platform","Language","N","Mismatching","Matching","Match Advantage"),
    align = c("l","l","r","r","r","r"),
   caption = "Median response times and accuracy percentages (in parentheses) per match condition (Mismatching, Matching); Match advantage (difference in response times) by language."
    )
#  in the on-site data
#  flextable() %>% 
# merge_v(j = ~Source) %>%  set_header_labels(Source = "Study Platform", Language = "Language", N = "N", Mismatch_stat ="Mismatching", Match_stat = "Matching", Effect = "Match Advantage")
```


```{r summary-osweb, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
SP_V_tidy %>% 
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
  filter(Outlier == FALSE & Source == "osweb") %>% ## Outlier by MAD
#  filter(Mark == FALSE & Source == "osweb") %>% ## Outlier by Shrinkage
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time), subject_ACC = n()/12) %>%
  group_by(Language, Match) %>%
  summarise(N = n(), med_RT = median(subject_M), mean_ACC = mean(subject_ACC)) %>%
  pivot_wider(names_from = Match, values_from = c(med_RT, mean_ACC)) %>%
  mutate(Effect = (med_RT_N - med_RT_Y) ) %>%
  transmute( N=N,
             Mismatch_stat = paste0(round(med_RT_N),"(",format(round(mean_ACC_N*100,2),nsmall=2),")"),
             Match_stat = paste0(round(med_RT_Y),"(",format(round(mean_ACC_Y*100,2),nsmall=2),")"),
             Effect = Effect) %>%
  kable(  
    format = "latex",
    booktabs = TRUE,
    escape=FALSE,
    col.names = c("Language","N","Mismatching","Matching","Match Advantage"),
    align = c("l","r","r","r","r"),
   caption = "Median reaction times and accuracy percentages (in parentheses) per match condition (Mismatching, Matching); Match advantage (difference in response times) by language in the web-based data."
    )
#  flextable() %>% 
#  set_header_labels(Language = "Language", N = "N", Mismatch_stat ="Mismatching", Match_stat = "Matching", Effect = "match advantage")
```


**Identification of outliers.** Our preregistered plan included excluding outliers based on a linear mixed-model analysis for participants in the third quantile of the grand intercept (i.e., participants with the longest average response times). Only `r round(100*sum(Prereg_N$Lab_N)/(sum(sum_site$SP_N) + sum(sum_osweb$SP_N)),2)` % of participants' data could pass this criterion. After examining the data from both online and in-person data collection, it became clear that both a minimum response latency and maximum response latency should be employed, as improbable times existed at both ends of the distribution [@kvalsethHickLawEquivalent2021; @proctorHickLawChoice2018]. The maximum response latency was calculated as two times the mean absolute deviation plus the median calculated separately for each participant. <!---Individual participants were removed if they did not reach our accuracy criterion, and ---> individual data points were excluded if they did not fall within the acceptable response time range.
`r sum(sum_site$SP_N) + sum(sum_osweb$SP_N) - sum(MAD_N$Lab_N)`

<!--All -the below data analysis depended on the datasets excluding the outliers. --->Table \@ref(tab:summary-languages) summarizes the match advantages by language from each data collection platform.

(Insert Table \@ref(tab:summary-languages) about here )


<!---For each laboratory, outliers were identified by the third quantile of the grand intercept in the simplest mixed-effects model. This mixed-effects model contained the response times as the dependent measure, matching condition as the only fixed effect, and the participant as the only random intercept. Among the data sets showing outliers, the averaged proportion of outliers was 0.25. Table S4 in Appendix 1 illustrates the distribution of outliers by laboratory. Table \@ref(tab:summary-site) and Table \@ref(tab:summary-osweb) respectively summarise the match advantages by language. 

(Insert Table \@ref(tab:summary-site) about here )

(Insert Table \@ref(tab:summary-osweb) about here )
--->


```{r meta_setup, message=FALSE, warning=FALSE, include=FALSE}
## Prepare the data sets for the meta analysis
## Merge the sources
SP_V_meta_data <- SP_V_tidy %>% 
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>%  ## Reserve the included data in each lab (by MAD)
#  filter(Mark == FALSE) %>%  ## Reserve the included data in each lab (by shrinkage)
  group_by(Language, PSA_ID, Subject,Match) %>%
##  summarise(RT = mean(response_time), ACC = 100*(sum(correct)/12)) %>%
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>%
  filter(ACC > .7) %>%
  pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y,na.rm=TRUE),m_mismatch=median(RT_N,na.rm=TRUE),
            sd_match=sd(RT_Y,na.rm=TRUE),sd_mismatch=sd(RT_N,na.rm=TRUE),
            acc_match=mean(ACC_Y,na.rm=TRUE),acc_mismatch=mean(ACC_N,na.rm=TRUE),
            ni=n()) %>%
  bind_cols(ri=.5)

## Only from site
SP_V_site_meta_data <- SP_V_tidy %>%
  filter(Source == "site") %>%
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>%  ## Reserve the included data in each lab (by MAD)
#  filter(Mark == FALSE) %>%  ## Reserve the included data in each lab (by shrinkage)
  group_by(Language,PSA_ID,Subject,Match) %>%
#  summarise(RT = mean(response_time), ACC = 100*(sum(correct)/12)) %>%
### Erin used medians in the revised code ###
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>%   pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5)

## Only from osweb
SP_V_osweb_meta_data <- SP_V_tidy %>%
  filter(Source == "osweb") %>%
  left_join(outliers_marks, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>%  ## Reserve the included data in each lab
#  filter(Mark == FALSE) %>%  ## Reserve the included data in each lab (by shrinkage)
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  group_by(Language,PSA_ID,Subject,Match) %>%
##  summarise(RT = mean(response_time), ACC = 100*(sum(correct)/12)) %>%
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>%   pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5)

SP_V_combine_meta_data <- bind_rows( bind_cols(Source = "site", SP_V_site_meta_data), bind_cols(Source="osweb",SP_V_osweb_meta_data))
```


**Meta-analysis of match advantages across laboratories.** Because the preregistered analysis plan did not consider the data collected online, we conducted the overall meta-analyses for the complete dataset and separately by data collection source.<!---all the datasets combined data sources. In this analysis, we computed the effect size by data set and estimated the global effect size.---> Since data from small samples may contribute to a biased estimate, nine laboratories<!---datasets---> with sample sizes smaller than 25 were excluded from the analyses[^1]. The overall meta-analysis did not find a <!---foundnoin--->significant match advantage. This result was regardless of whether the analysis was run on the whole dataset, or separately by data source (on-site or web-based) (see Figure \@ref(fig:meta-all)). <!---Among the languages that had at least two laboratories datasets, we conducted the meta-analysis for English, German, Norway, Traditional Chinese, Slovak, and Turkey. Only <!---Traditional Chinese--->German showed a significant meta-analytic effect across laboratories(see Figure \@ref(fig:meta-ger)).---> Results of the other languages are available in Appendix 3.

^1: Some laboratories requested withdrawal before they collected the requested minimum 50 participants for the unexpected affairs. The first team (GBR_006) stopped the data collection at 25th participant.

```{r meta-all, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE, fig.cap="Meta-analysis on match advantage of object orientation for all datasets"}


source("includes/files/meta_all_plot.R")

```

<!---
(Insert Figure \@ref(fig:meta-all) about here)
--->


```{r meta-sources, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}

## , fig.cap="Meta-analysis on match advantage of object orientation for all datasets by sources."

## Merge meta analysis data from two sources
es_data <- SP_V_site_meta_data %>%
  filter((ni > 25)) %>% ## Exclude small sample size
  cbind(Source = "site") %>%
bind_rows(
SP_V_osweb_meta_data %>%
  filter((ni > 25)) %>% ## Exclude small sample size
  cbind(Source = "osweb")
)


SP_V_sources_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=es_data)
SP_V_meta_sources <-  rma.uni(yi, vi, data = SP_V_sources_es, slab = PSA_ID, method = "REML", digits = 2)

## Because Rmd can not alter base::plot, the below codes were moved to one single R script.
source("includes/files/meta_sources_plot.R")

#forest(SP_V_meta_sources, header ="Team ID          N", ilab= ni, ilab.xpos = -270, ylim=c(-1,60), rows=c(4:19, 27:55), mlab = mlabfun("RE model for All Teams", SP_V_meta_sources) )

#op <- par(cex=0.75, font=2)


### add text for the sources
#par(font=4)
#text(-300, c(56,20), pos=4, c("On Site", "On Internet"))

#par(op)

### fit random-effects model in the two sources
#SP_V_site_es <- escalc(measure = "MC", 
#         m1i = m_mismatch, m2i = m_match, 
#         sd1i = sd_mismatch, sd2i = sd_match, 
#         ni = ni, ri = ri, 
#         slab = PSA_ID, data=site_es_data)
#SP_V_meta_site <-  rma.uni(yi, vi, subset = (Source=="site"), data = SP_V_sources_es, slab = PSA_ID, method = "REML", digits = 2)

#SP_V_osweb_es <- escalc(measure = "MC", 
#         m1i = m_mismatch, m2i = m_match, 
#         sd1i = sd_mismatch, sd2i = sd_match, 
#         ni = ni, ri = ri, 
#         slab = PSA_ID, data=es_data)
#SP_V_meta_osweb <-  rma.uni(yi, vi, subset = (Source=="site"), data = SP_V_sources_es, slab = PSA_ID, method = "REML", digits = 2)
##forest(SP_V_meta, mlab = "Data sets from sites")

### add summary polygons for the two subgroups
#addpoly(SP_V_meta_site, row=22, mlab=mlabfun("RE Model for On site data", SP_V_meta_site))
#addpoly(SP_V_meta_osweb, row= 2, mlab=mlabfun("RE Model for on Internet data", res.r))
```


(Insert Figure \@ref(fig:meta-sources) about here)



<!---
The meta-analysis of the lab-based data showed a match advantage with small effect size and little variation among laboratories. Only one laboratory (HUN 001) found a significant match advantage (Figure \@ref(fig:meta-site)).

(Insert Figure \@ref(fig:meta-site) about here)
--->

<!---
The meta-analysis of the Internet-based data revealed a match disadvantage with small effect size. Only one laboratory (NZL 005) found a match advantage (Figure \@ref(fig:meta-osweb)). There was greater variation among lab-based datasets than the Internet-based datasets.  

(Insert Figure \@ref(fig:meta-osweb) about here)
--->




```{r meta-ger, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Meta-analysis on match advantage of object orientation for German datasets."}
## Locate English data for meta-analysis
SP_V_ger_meta_data <- SP_V_tidy %>%
  filter(Language == "German") %>%
##  left_join(outliers_table, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>%  ## Reserve the included data in each lab
  group_by(Language,PSA_ID,Subject,Match) %>%
#  summarise(RT = mean(response_time), ACC = 100*(sum(correct)/12)) %>%
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>% 
  pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5) %>%
  filter((ni > 25)) ## Exclude lab less than 25 participants

## Prepare the elements for computing the meta-analytic effect size
SP_V_ger_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=SP_V_ger_meta_data)

## Compute the meta-analytic effect size
SP_V_meta <-  rma.uni(yi, vi, data = SP_V_ger_es, slab = PSA_ID, method = "REML", digits = 2)

## Output the forest plot
forest(SP_V_meta, mlab = "")
```



(Insert Figure \@ref(fig:meta-ger) about here)




```{r SP-lme-data, message=FALSE, warning=FALSE, include=FALSE}
# export data file for the making of appendix
# SP_V_lme_data <- dir(path = "..",
#      pattern = "SP_V_lme_data.csv",   ## include in-site and internet data
#      recursive = TRUE, full.names = TRUE) %>% 
#      read_csv() %>%
#  inner_join(subset(outliers_table, Outlier == FALSE),by = c("Subject","PSA_ID" = "LAB"))

  
# SP_V_lme_data$Language <- ifelse(SP_V_lme_data$Language == "Magyar","Hungrian",SP_V_lme_data$Language)

# SP_V_lme_data <- mutate(SP_V_lme_data,
#                        Match = factor(Match,
#                          levels = c("Y","N"),
#                          labels = c("MATCHING","MISMATCHING")),
#                        Source = factor(Source, 
#                       levels = c("site","osweb"),
#                       labels = c("Site","Internet") ))

## Erin blocked the above code. There might be no data for the appendix.


SP_V_lme_data <- SP_V_tidy %>% 
##   left_join(outliers_marks, by = c("PSA_ID" = "LAB", "Subject" = "Subject")) %>% ## outliers_makrs was from cycle_outlier; disable for the following analysis
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>%  ## excluded outliers by MAD
#  filter(Mark == FALSE) %>%  ## excluded outliers by shrinkage
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  mutate(Match = factor(Match,
                        levels = c("Y","N"),
                        labels = c("MATCHING","MISMATCHING")),
                        Source = factor(Source, 
                        levels = c("site","osweb"),
                        labels = c("Site","Internet") ))

# Export data for app4
write_csv(SP_V_lme_data, file="includes/files/SP_V_lme_data.csv")
```



```{r SP-source-lme, message=FALSE, warning=FALSE, include=FALSE}
## analysis to decide if we had to analyze on-site and web-based respectively

SP_V_lme_data$r_Source = if_else(SP_V_lme_data$Source == "Site",1,0)


source_cor.lmer <- lmerTest::lmer(
  response_time ~ Match*r_Source + 
    (1|Subject) + 
    (r_Source|PSA_ID) + 
    (r_Source|Language), 
  control = lmerControl(optimizer = "bobyqa",
                        optCtrl = list(maxfun = 1e6)), 
  data = SP_V_lme_data)

source_cor_lmer_out <- round(summary(source_cor.lmer)$coefficients["r_Source",],3)
```

```{r SP-sensitive, message=FALSE, warning=FALSE, include=FALSE}
coef_all <- as.data.frame(summary(source_cor.lmer)$coefficients)
coef_all$sig_t <- qt(.025, coef_all$df, lower.tail = F)
coef_all$min_effect <- coef_all$sig_t * coef_all$`Std. Error`

coef_all[ , c(1,7)]
```

**Evaluating match advantages using linear mixed-effects models.**  <!--- Considering the bias of small sample size, we excluded data from the languages with below less than 25 participants in each data source  (Portuguese – on-site; Norwegian – web-based) before conducting the mixed-effects models. Thus we excluded Portuguese in the on-site data and Norwegian in the web-based data, the sources of data collection included the labs and the web were varied, we had to evaluate evaluated whether one mixed-effects model sufficiently fitted all the data. Otherwise, separate models would be needed for each data set. --->All models presented in this section are reported in Appendix 4. We at first evaluated difference between on-site and web-based data in the mixed-effects model with the interaction of match condition and data source. This analysis showed no difference between data sources: `r paste0("*b* = ", source_cor_lmer_out["Estimate"], ", *SE* = ",source_cor_lmer_out["Std. Error"],", t( ",source_cor_lmer_out["df"]," ) = ",source_cor_lmer_out["t value"], ", *p* = ", source_cor_lmer_out["Pr(>|t|)"])`. Therefore<!---us--->, the following analysis did not separate on-site and the web-based data. <!--- The final models examined the interaction between language and match advantage. in each data source, as reported below.It must be acknowledged that the languages with larger sample sizes (see Tables \@ref(tab:summary-site) and \@ref(tab:summary-osweb)) have more reliable results. Furthermore, most of the languages were underpowered, being far from the 1,2001,000 participants suggested by an a priori power analysis. --->



```{r SP-lang-lme, message=FALSE, warning=FALSE}
## Check sample size of a language by team data
#site_excluded_lang <- subset(SP_V_lme_data, Source=="Site") %>%
team_excluded_lang <- SP_V_lme_data %>%
  group_by(Language, Subject) %>%
  summarise(N_trials = n()) %>%
  group_by(Language) %>%
  summarise(N = n()) %>%
  filter(N < 25) %>%
  pull(Language)  ## Excluded Portuguese because there was only one team and they stopped data collection due to global pandemics.

## Allocate the site data (block because of no diff. between site and osweb)
###SP_V_site_lme_data = subset(SP_V_lme_data, Source=="Site" & !(Language %in% site_excluded_lang))

SP_V_lme_data = subset(SP_V_lme_data, !(Language %in% team_excluded_lang) )

## Check the fittest model

### earliest code before first sumbission 2021 Nov
#m_Rsubject <- lmer(response_time ~ Language*Match + (1|Subject), data = SP_V_lme_data)

#m_Rsubject_Rtarget <- lmer(response_time ~ Language*Match + (1|Subject) + (1|Target), data = SP_V_lme_data)

#anocAIC(m_Rsubject, m_Rsubject_Rtarget) ## Best fit model has random intercepts of particpants and items.

## Erin's update
#only intercept
intercept.model <- lm(response_time ~ 1, 
                      data = SP_V_lme_data)
#add random intercept of subject
subject.model <- lmer(response_time ~ 1 + (1|Subject), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)
# add random intercept of item
item.model <- lmer(response_time ~ 1 + (1|Subject) + (1|Target), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)
# add random intercept of lab
lab.model <- lmer(response_time ~ 1 + (1|Subject) + (1|Target) + (1|PSA_ID), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)

# add random intercept of language
language.model <- lmer(response_time ~ 1 + (1|Subject) + (1|Target) + (1|PSA_ID) + (1|Language), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)

## which is best fit
#AIC(subject.model) < AIC(intercept.model)  ## TRUE
#AIC(item.model) < AIC(subject.model)       ## TRUE
#AIC(lab.model) < AIC(item.model)           ## TRUE
#AIC(language.model) < AIC(lab.model)       ## TRUE
section_01_AIC <- AIC(language.model) ## presentation in Result

section_01_BIC <- BIC(language.model)

## language.model is the best fittest model
# add fixted effect of match0
fixed.two.model <- lmer(response_time ~ Match + (1|Subject) + (1|Target) , 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)


fixed.four.model <- lmer(response_time ~ Match + (1|Subject) + (1|Target) + (1|PSA_ID) + (1|Language) , 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)

#AIC(fixed.model) < AIC(item.model)  ## TRUE
#AIC(fixed.model) < AIC(lab.model)   ## FALSE
#AIC(fixed.model) < AIC(language.model)  ## FALSE
section_02A_AIC <- AIC(fixed.two.model) ## presentation in Result
section_02B_AIC <- AIC(fixed.four.model) ## presentation in Result

# section_02B_AIC < section_02A_AIC ## fixed.four.model with Match is the fittest model

fixed.randomslope.model <- lmer(response_time ~ Match + (1|Subject) + (1|Target) +(1|PSA_ID) + (1 + Match|Language), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data)


#AIC(fixed.randomslope.model) < AIC(fixed.model)  ## FALSE
#AIC(fixed.randomslope.model) < AIC(item.model)   ## TRUE
#AIC(fixed.randomslope.model) < AIC(language.model)  ## FALSE

section_03_AIC <- AIC(fixed.randomslope.model) ## presentation in Result
section_03_BIC <- BIC(fixed.randomslope.model) ## presentation in Result


#section_03_AIC < section_02B_AIC ## fixed.four.model is better fit than fixed.raondomslope.model


## Export the stat info in the article
SP_lme00_out <- round(summary(fixed.two.model)$coefficients["MatchMISMATCHING",],3)
SP_lme01_out <- round(summary(fixed.four.model)$coefficients["MatchMISMATCHING",],3)
SP_lme_out <- round(summary(fixed.randomslope.model)$coefficients["MatchMISMATCHING",],3)
```

```{r SP-lang-sensitive, message=FALSE, warning=FALSE, include=FALSE}
coef_all_site <- as.data.frame(summary(fixed.randomslope.model)$coefficients)
coef_all_site$sig_t <- qt(.025, coef_all_site$df, lower.tail = F)
coef_all_site$min_effect <- coef_all_site$sig_t * coef_all_site$`Std. Error`

coef_all_site[ , c(1,7)]
# approximately 2 seconds no matter what 
```



<!---In each data source, w--->At the next step, we decided the model with all random intercepts (participants, items, laboratories, and languages) in terms of the smallest AIC = `r section_01_AIC`. In comparion with the models with matching condition as the fixed effect, the model with the random intercepts of participants and items (AIC = `r section_02a_AIC`) had better fit than the model with all random intercepts (AIC = `r section_02_AIC`). Neither the model with two random intercepts,  `r paste0("*b* = ", SP_lme00_out["Estimate"], ", *SE* = ",SP_lme00_out["Std. Error"],", t( ",SP_lme00_out["df"]," ) = ",SP_lme00_out["t value"], ", *p* = ",SP_lme00_out["Pr(>|t|)"])`, nor the model with all random intercepts revealed significant effect of match advantage: `r paste0("*b* = ", SP_lme01_out["Estimate"], ", *SE* = ",SP_lme01_out["Std. Error"],", t( ",SP_lme01_out["df"]," ) = ",SP_lme01_out["t value"], ", *p* = ",SP_lme01_out["Pr(>|t|)"])`. Among the other considering models, the highest theoretical interested model had a random slope of matching condition on language (AIC = `r section_03_AIC`). <!---with and without the random intercept slope of items matching condition.   Both The result indicated that the models with out  the random intercept slope had the best fitness. The model from the on-site dataThus we evaluated the match advantage in terms of the model with the random intercepts. --->This model also revealed no significant effect of match advantage: `r paste0("*b* = ", SP_lme_out["Estimate"], ", *SE* = ",SP_lme_out["Std. Error"],", t( ",SP_lme_out["df"]," ) = ",SP_lme_out["t value"], ", *p* = ",SP_lme_out["Pr(>|t|)"])` <!---and no interaction of match advantage and any language, all *ps* > .05. The model from the web-based data also failed to reveal a significant effect: . The latter model had a negative coefficient, unlike the on-site data. the difference in direction resounds with the match advantages and disadvantages found in experiments using the property of color [cf. @connellRepresentingObjectColour2007; @zwaanRevisitingMentalSimulation2012].---> <!---Figure \@ref(fig:plot-SP-site-lme) illustrates the response times from the on-site data.---> <!--- presented significant intercepts---> (see “Models included languages” section in Appendix 4). The illustration of match advantages by language showed the more participants a language recruited the smaller standard error we measured, although no language showed a significant effect of match advantage (see Figure \@ref(fig:plot-SP-lme-coef) ).  

```{r plot-SP-lme-raw, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# cap = Average response latency by language and matching condition.
# plot of the data for just raw score differences
ggplot(SP_V_lme_data, aes(Language, response_time, shape = Match)) + 
  theme_classic() + 
  ylab("Response Latencies") + 
  xlab("Language") + 
  stat_summary(fun = mean, 
               geom = "point") + 
  stat_summary(fun.data = mean_cl_normal, 
               geom = "pointrange") + 
  theme(axis.text.x = element_text(angle = 90))
```


(Insert Figure \@ref(fig:plot-SP-lme-coef) about here)

```{r plot-SP-lme-coef, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Average Response times and 95% CI in the sentence-picture verification task by match condition in each language"}
coef_model <- coef(fixed.randomslope.model)$Language
coef_model$MATCHING <- coef_model$`(Intercept)`
coef_model$MISMATCHING <- coef_model$MATCHING + coef_model$MatchMISMATCHING
coef_model$Language <- rownames(coef_model)

coef_model <- coef_model %>% left_join(
SP_V_lme_data %>% count(Language, Subject) %>%
  group_by(Language) %>%
  summarise(N = n()) %>%
  mutate(Size = ifelse(N > 1000,5,
                       ifelse(N<=1000 & N > 250,4,
                              ifelse(N<=250 & N >150, 3,
                                     ifelse(N<=150 & N>50,2, 1)))))
,by = "Language")

library(parameters)
se_model <- as.data.frame(standard_error(fixed.model, effects = "random")$Language)
coef_model$se <- se_model$`(Intercept)`
 

coef_data <- coef_model %>% 
  pivot_longer(cols = c("MATCHING", "MISMATCHING")) %>% 
  mutate(lower = value + qnorm(.025)*se,   ## Compute CI for illustration 
         upper = value + qnorm(.925)*se) %>% 
  rename(Match = name)

ggplot(coef_data, aes(Language, value, shape = Match)) + 
  theme_classic() + 
  ylab("Response Latencies") + 
  xlab("Language") + 
  geom_point(aes(size = Size)) +
  scale_size("Number of Participants") +
  scale_size_continuous(
    labels = c("<= 50","51 ~ 150","151~ 250","250~1000",">1,000")
  ) +
  geom_pointrange(data = coef_data, aes(ymin = lower, ymax = upper)) +
  theme(axis.text.x = element_text(angle = 90))
```



```{r plot-SP-site-lme, eval=FALSE, fig.cap="Response", message=FALSE, warning=FALSE, include=FALSE}
# Plot interaction
sjPlot::set_theme(axis.angle.x = 45,
                  axis.textsize = .8)

sjPlot::plot_model(site_cor.lmer, 
                   type = "eff", 
                   terms = c('Language', 'Match'), 
                   #ci.lvl = .95,
                   se = TRUE,
                    colors = "gs") +
  ylab("Response Time(ms)") + 
  labs(title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))

```


```{r plot-SP-osweb-lme, eval=FALSE, fig.cap="Response", message=FALSE, warning=FALSE, include=FALSE}
# Plot interaction
#sjPlot::set_theme(axis.angle.x = 45,
#                  axis.textsize = .8)

#sjPlot::plot_model(osweb_cor.lmer, 
#                   type = "eff", 
#                   terms = c('Language', 'Match'), 
                #   ci.lvl = .95,
#                   se = TRUE,
#                    colors = "gs") +
#  ylab("Response Time(ms)") + 
#  labs(title = "") + 
#  theme(panel.grid.major = element_blank(),
#        panel.grid.minor = element_blank(),
#        panel.background = element_blank(), 
#        axis.line = element_line(colour = "black"))
### firstup(xfun::numbers_to_words(sum(summary(osweb_cor.lmer)$coefficients[2:14,"Pr(>|t|)"] < .05)))

##  (*M* = `r round(mean(subset(SP_V_lme_data, Language=="Greek") %>% pull(response_time)),2)`, *SD* = `r round(sd(subset(SP_V_lme_data, Language=="Greek") %>% pull(response_time)),2)`) and Serbian (*M* = `r round(mean(subset(SP_V_lme_data, Language=="Serbian") %>% pull(response_time)),2)`, *SD* = `r round(sd(subset(SP_V_lme_data, Language=="Serbian") %>% pull(response_time)),2)`) was longer than the average across languages (M = `r round(mean(SP_V_lme_data$response_time),2)`, SD = `r round(sd(SP_V_lme_data$response_time),2)`)
```



<!--- 
Figure \@ref(fig:plot-SP-osweb-lme) illustrates the response times in the web-based data.  languages presented significant effects (see “Models included languages” section in Appendix 4).

(Insert Figure \@ref(fig:plot-SP-osweb-lme) about here)

Removed this section because Greek data had no effect after we excluded the shortest response times. --->
<!---**Anecdotal evidence on the match advantage.** In the on-site data, only Greek presented a match advantage, . It should be noted, however, that these results are not robust due to the underpowered sample sizes (see Discussion). The mean median response times in Greek. This might not be coincidental, as according to @yapRespondingNonwordsLexical2014, longer response times have been associated with larger effects in psycholinguistics [@schillingComparingNamingLexical1998; @seidenbergTimeCoursePhonological1985; @tainturierEducationalLevelWord1992]. (**This paragraph requires the further discussion.**)  --->

```{r PP_data_preparation, message=FALSE, warning=FALSE}
## Dataset for mixed-effect model
PP_lme_data <- PP_tidy %>% 
#  left_join(outliers_table, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%  ## filter the outliers by SP_V data
  filter(Outlier == FALSE) %>% #filter outlier by time rule established above
  # consider excluding people who couldn't get these right 
  # using the same accuracy levels as above 
  left_join(rbind(PP_subj_osweb, PP_subj_site), 
             by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(P_Acc >= .70) %>%
  mutate(Identical= factor(Identical,
                          levels = c("Y","N"),
                          labels = c("SAME","DIFF")))

## export data for app5
write_csv(PP_lme_data, file="includes/files/PP_lme_data.csv")
```



```{r PP_source_lme, message=FALSE, warning=FALSE}
PP.intercept.lme <- lm(response_time ~ 1,
                    data = PP_lme_data
                    ) 

PP.subject.lme <- lmerTest::lmer(response_time ~ 1 +
                    (1 | Subject),   # By-subject random intercept
             data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 


PP.item.lme <- lmerTest::lmer(response_time ~ 1 +
                    (1 | Subject) +   # By-subject random intercept
                    (1 | Picture1),    # By-item random intercept
             data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 


PP.lab.lme <- lmerTest::lmer(response_time ~ 1 +
                    (1 | Subject) +   # By-subject random intercept
                    (1 | Picture1) +   # By-item random intercept
                    (1 | PSA_ID),    # By-lab random intercept
                    data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 


#AIC(PP.subject.lme) < AIC(PP.intercept.lme)  ## PP.subject.lme is better
#AIC(PP.item.lme) < AIC(PP.subject.lme) ## PP.item.lme is better
#AIC(PP.lab.lme) < AIC(PP.item.lme) ## PP.lab.lme is better
PP_lab_AIC <- AIC(PP.lab.lme) # Presentation in text
PP_lab_BIC <- BIC(PP.lab.lme) # Presentation in text

## Erase the analysis on data sources
#PP.source.lme <- lmerTest::lmer(response_time ~ 
#                   Source +                # Fixed effect
#                    (1 | Subject) +   # By-subject random intercept
#                    (1 | Picture1) +   # By-item random intercept
#                    (1 | PSA_ID),   # By-lab random intercept
#                    data = PP_lme_data,
#                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
#                    ) 

## 
#pp_source_out <- round(summary(PP.source.lme)$coefficients["Sourcesite",],3)


```


```{r PP_lang_lme, message=FALSE, warning=FALSE}

PP.lang.add.cor.lme <- lmerTest::lmer(response_time ~ 
                    Identical+Language +                # Fixed effect
                    (1 | Subject) +   # By-subject random intercept
                    (1 | Picture1) +   # By-item random intercept
                    (1 | PSA_ID),    # By-lab random intercept
                    data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 


PP.lang.inter.cor.lme <- lmerTest::lmer(response_time ~ 
                    Identical*Language +                # Fixed effect
                    (1 | Subject) +   # By-subject random intercept
                    (1 | Picture1) +   # By-item random intercept
                    (1 | PSA_ID),    # By-lab random intercept
                    data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 

PP_add_AIC <- AIC(PP.lang.add.cor.lme)
PP_inter_AIC <- AIC(PP.lang.inter.cor.lme)

#PP_inter_AIC < PP_add_AIC # PP.lang.inter.cor.lme is better

pp_cor_lme_out <- round(summary(PP.lang.inter.cor.lme)$coefficients["IdenticalDIFF",],3)

#reported_p <- ifelse(pp_cor_lme_out["Pr(>|t|)"] < .001,"< .001", paste0("= ", pp_cor_lme_out["Pr(>|t|)"]))

```

```{r PP-sensitive, message=FALSE, warning=FALSE, include=FALSE}
coef_PP <- as.data.frame(summary(PP.lang.inter.cor.lme)$coefficients)
coef_PP$sig_t <- qt(.025, coef_PP$df, lower.tail = F)
coef_PP$min_effect <- coef_PP$sig_t * coef_PP$`Std. Error`

mean(coef_PP[20:36 , "min_effect"])
```


**Analysis of imagery scores.** In our preregistered plan, we claimed ANOVA on the interaction of language and imagery score based on the assumption<!---ed---> that the object orientation settings (same, different) across every language group would be nearly equal. Because the later data collection was on Internet, we used mixed models instead of ANOVA to evaluate the all data sets. Among the considering models without fixed effect, the best fitted model had the random intercepts of participants, items, and laboratories (AIC = `r PP_lab_AIC`). Added the data sources into the model as the fixed effect, the model revealed no difference between the data sources, `r paste0("*b* = ", pp_source_out["Estimate"], ", *SE* = ",pp_source_out["Std. Error"],", t( ",pp_source_out["df"]," ) = ", pp_source_out["t value"], ", *p* = ",pp_source_out["Pr(>|t|)"])`. 

Included language and object orientation settings as the fixed effects, the models with the interaction of two effects (AIC = `r PP_add_AIC`) better fitted the data than the model with the addition of two effects (AIC = `r PP_inter_AIC`). The interaction model indicated that the fixed effect of imagery scores<!---orientation match---> was significant, `r paste0("*b* = ", pp_cor_lme_out["Estimate"], ", *SE* = ",pp_cor_lme_out["Std. Error"],", t( ",pp_cor_lme_out["df"]," ) = ",pp_cor_lme_out["t value"], ", *p* ", ifelse(pp_cor_lme_out["Pr(>|t|)"] < .001,"< .001", paste0("= ", pp_cor_lme_out["Pr(>|t|)"])))`. The response times illustrated in Figure \@ref(fig:plot-PP-lme) indicated that the imagery scores measured for each language were consistently positive supporting our hypothesis. The coefficients of all evaluated mixed-effects models are reported in Appendix 5. 

(Insert Figure \@ref(fig:plot-PP-lme) about here) 


```{r plot-PP-lme, message=FALSE, warning=FALSE, fig.cap="Response times and standard error in the picture-picture verification task by match condition in each language (both on-site and web-based data)."}
# Plot interaction
sjPlot::set_theme(axis.angle.x = 45,
                  axis.textsize = .8)

sjPlot::plot_model(PP.lang.inter.cor.lme, 
                   type = "eff", 
                   terms = c('Language', 'Identical'), 
                #   ci.lvl = .95,
                   se = TRUE,
                    colors = "gs") +
  ylab("Response Time(ms)") + 
  labs(title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))

```



```{r prediction_data, message=FALSE, warning=FALSE}
## Dataset for traditional ANOVA
PP_aov_data <- PP_tidy %>% #left_join(outliers_table, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%  ## filter the outliers by SP_V data
  filter(Outlier == FALSE) %>%#filter outlier by time rule established above
  # consider excluding people who couldn't get these right 
  # using the same accuracy levels as above 
#mutate(Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) %>%
  left_join(rbind(PP_subj_osweb, PP_subj_site), 
             by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(P_Acc >= .70) %>% 
  group_by(Source, Language, PSA_ID, Subject, Identical) %>%
  summarise(subject_M = median(response_time), subject_ACC = mean(P_Acc))

## I will merge the SP_V and PP data by participants' mean response times
##model_site_data <- (SP_V_tidy %>% 
model_data <- (SP_V_tidy %>% 
#                      left_join(outliers_table, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
#  filter(Language != site_excluded_lang & Outlier == FALSE & Source == "site") %>%
##  mutate(Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
#  filter(Language != site_excluded_lang) %>% 
#  filter(Source == "site") %>% 
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time)) %>%
#  group_by(Language, Match) %>%
#  summarise(N = n(), med_RT = median(subject_M), mean_ACC =   mean(subject_ACC)) %>%
  pivot_wider(names_from = Match, values_from = c(subject_M)) %>%
  mutate(Effect = (N - Y) )) %>%
left_join(
(PP_aov_data %>%
  select(-subject_ACC) %>%
  pivot_wider(names_from = Identical, values_from = subject_M) %>%
  mutate(Imagery = (N - Y))),
by = c("Language","Subject")
)


#model_osweb_data <- (SP_V_tidy %>%
#                       left_join(outliers_table, by=c("PSA_ID" = "LAB", "Subject" = "Subject")) %>%
#  filter(Language != osweb_excluded_lang & Outlier == FALSE & Source == "osweb" ) %>%
##  mutate(Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) %>%
#  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
#            by = c("Subject" = "Subject", 
#                   "Language" = "Language", 
#                   "Match" = "Match",
#                   "PSA_ID" = "PSA_ID")) %>% 
#  filter(Outlier == FALSE) %>% 
#  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
#  filter(Language != site_excluded_lang) %>% 
#  filter(Source == "osweb") %>% 
#  group_by(Language, Subject, Match) %>%
#  summarise(subject_M = median(response_time)) %>%
#  group_by(Language, Match) %>%
#  summarise(N = n(), med_RT = median(subject_M), mean_ACC =   mean(subject_ACC)) %>%
#  pivot_wider(names_from = Match, values_from = c(subject_M)) %>%
#  mutate(Effect = (N - Y) )) %>%
#left_join(
#(PP_aov_data %>%
#  select(-subject_ACC) %>%
#  pivot_wider(names_from = Identical, values_from = subject_M) %>%
#  mutate(Imagery = (N - Y))),
#by = c("Language","Subject")
#)
```


```{r prediction_model, message=FALSE, warning=FALSE, include=FALSE}
## Prediction models for all languages 
lang_model1 <- lm(Effect ~ Language*Imagery, data=model_data)
lang_model0 <- lm(Effect ~ Language, data=lang_model1$model)
model_test <- anova(lang_model0, lang_model1)[2,c("Res.Df","F","Df","Pr(>F)")]
lang_reults <- apa_print(lang_model0)
imagery_results <- apa_print(lang_model1)

Ger_model0 <- lm(Effect ~ Imagery,
data=subset(model_data, Language == "German"))

Ger_result <- summary(Ger_model0)$coef["Imagery",]
```

<!---The above analyses suggested that data sources did not influence the imagery scores but did influence the match advantage. Therefore, we compared evaluated the fit of the model of with languages and imagery scores and the model with languages only. Both models included match advantage as the dependent variable.---> The last preregistered plan was to build the regression model to predict the match advantages. The regression model depended on the average results of match advantage and imagery scores. If imagery scores predicted match advantage, the regression model with languages and imagery scores should fit the data better than the regression model with languages only. Because the overall match advantages are around zero, we test the prediction capability of imagery scores for the Germany data only. The analysis indicated a negative but insignificant coefficent, *b* = `r Ger_result["Estimate"] %>% round(2)`, *SE* =  `r Ger_result["Std. Error"] %>% round(2)`, *p* = `r Ger_result["Pr(>|t|)"]`. 


<!---Because the random slopes for items in the analyses of the match advantagewere zero (see Appendix 5), the data for building the regression models were the aggregated data by participants. --->

<!---In the linear regression analysis, we  selected the best fit model from the model with for only one predictor, language, and the model with two predictors, language and imagery scores. Because the analysis of match advantage revealed the a no difference between on-site versus web-based data, we conducted separate one set of the regression analysis for the combination of two data sources. In this e analysis of the on-site data, the model with language only and imagery scores had yet fit the data as good as better than the model with language and imagery scores only, `r paste0("*F* (",model_test["Df"],",",model_test["Res.Df"],") = ",round(model_test["F"],3),", *p* =", format(round(model_test["Pr(>F)"],3),nsamll=3) )`. In contrast, in the analysis of the web-based data , the model with language and imagery scores had a better fit than also indicated the model with language only had the best fitness, . In the latter case, the effect of imagery scores was nonsignificant, . The imagery scores obviously was not a reliable predictor for the match advantage of object orientation. (**Revised summary; require the co-authors' comments**) Appendix 5 summarized the coefficients of the models included in these analyses. --->

## Exploratory analysis: Language-specific match advantages


```{r Eng_effect01_lme, message=FALSE, warning=FALSE, include=FALSE}
SP_V_eng_tidy <- SP_V_lme_data %>%
  filter(Language == "English")
#SP_V_eng_tidy$r_System = if_else(SP_V_eng_tidy$System == "US",1,0)
SP_V_eng_tidy$r_System <- if_else(
  grepl("PSA|USA", SP_V_eng_tidy$PSA_ID),
  1, 0)

#SP_V_eng_tidy$r_Source = if_else(SP_V_eng_tidy$Source == "Site",1,0)

eng_no_slope.lmer = lmerTest::lmer(response_time ~ Match*r_System + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_eng_tidy)

eng_AIC <- AIC(eng_no_slope.lmer)

```



```{r GER_effect01_lme, message=FALSE, warning=FALSE, include=FALSE}
SP_V_GER_tidy <- SP_V_lme_data %>% 
  filter(Language == "German")

#SP_V_GER_tidy$r_Source = if_else(SP_V_GER_tidy$Source == "Site",1,0)

GER_random.lmer = lmerTest::lmer(response_time ~ 1 + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_GER_tidy)

GER_no_slope.lmer = lmerTest::lmer(response_time ~ Match + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_GER_tidy)

summary(GER_no_slope.lmer)

#GER_no_slope_inter.lmer = lmerTest::lmer(response_time ~ Match*r_Source + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_GER_tidy)
##summary(GER_no_slope_inter.lmer)
##AIC(GER_no_slope.lmer) < AIC(GER_random.lmer) ### TRUE

GER_AIC <- AIC(GER_no_slope.lmer)  ## presented in Result
GER_BIC <- BIC(GER_no_slope.lmer)  ## presented in Result

GER_lme_out <- round(summary(GER_no_slope.lmer)$coefficients["MatchMISMATCHING",],3)

```


Based on our preregistered exploratory plan<!---described earlierthe policy to conduct the linguistic-specific mixed-effect models--->, we selected the English datasets ($N$ = `r format(length(unique(SP_V_eng_tidy$Subject)), scientific=FALSE, big.mark = ",")`) and the <!---Traditional Chinese--->Germany datasets ($N$ = `r format(length(unique(SP_V_GER_tidy$Subject)), scientific=FALSE, big.mark = ",")`). For both languages, we are interested in whether the data sources would show differences in the<!---could inhibit the --->match advantage. Another topic of interest is if the match advantage changed with English dialects, namely American English and British English.


Using the data from `r format(length(unique(SP_V_eng_tidy$Subject)), scientific=FALSE, big.mark = ",")` English speaking participants, we conducted a mixed-effects model using<!---for the English data containing orientation --->match condition<!---,---> and English dialects (American vs. British)<!--- and data sources (on-site vs. web-based)---> as fixed effects. The <!---fittest---> model  with the best model fit (AIC = `r eng_AIC`) indicated the difference between the dialects, `r paste0("*b* = ", round(summary(eng_no_slope.lmer)$coefficients["r_System","Estimate"],3), ", *SE* = ",round(summary(eng_no_slope.lmer)$coefficients["r_System","Std. Error"],3),", t( ",round(summary(eng_no_slope.lmer)$coefficients["r_System","df"],3)," ) = ",round(summary(eng_no_slope.lmer)$coefficients["r_System","t value"],3), ", *p* = ", format(round(summary(eng_no_slope.lmer)$coefficients["r_System","Pr(>|t|)"],3), nsmall=3))`, but failed to reveal the match advantage, `r paste0("*p* = ", format(round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING","Pr(>|t|)"],3), nsmall=3) )`.  <!---Following @brauerLinearMixedeffectsModels2018, English dialects and data sources were numerically recoded. The best fitting ed model indicated that only data source (on-site vs. web-based) was significant, . <!---Although the match advantage of orientation was nonsignificant--->Also, this exploratory analysis indicated <!---the--->null interaction of orientation match condition and English dialects: `r paste0("*p* = ",format(round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","Pr(>|t|)"],3), nsmall=3) )`  (see the detailed report in Appendix 4). 

<!---
Figure \@ref(fig:Eng-interaction-plot) showed the match advantages happened in the Internet data but only for American English.

(Insert Figure \@ref(fig:Eng-interaction-plot) about here)
--->

```{r Eng-interaction-plot, eval=FALSE, fig.cap="Estimated", message=FALSE, warning=FALSE, include=FALSE}
# Plot interaction

sjPlot::plot_model(eng_no_slope.lmer, 
                   type = "eff", 
                   terms = c('Match','r_System' #'r_Source', ), 
                   #ci.lvl = .95,
                   se = TRUE,
                   colors = "gs",
                   show.legend = FALSE) +
  ylab("Response Time(ms)") + 
  labs(title = "",axis.title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        strip.background = element_blank(),
        strip.text = element_blank(),
        axis.line = element_line(colour = "black"))
```


We conducted another exploratory mixed-effect model on <!---Traditional Chinese---> Germany data because his was the only language to show a significant result in the preregistered meta-analysis. The <!---best fit--->model with the best fit (AIC = `r GER_AIC`) had orientation match condition and data sources as the fixed effects. <!---This model indicated that data source was significant, ---> The match advantage of orientation was far from the significant level, `r paste0("*b* = ", round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING","Estimate"],3), ", *SE* = ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING","Std. Error"],3),", t( ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING","df"],3)," ) = ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING","t value"],3), ", *p* = ",format(round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING","Pr(>|t|)"],3)),nsmall=3)`, and the interaction of match advantage and data sources was nonsignificant, `r paste0("*b* = ", round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING:r_Source","Estimate"],3), ", *SE* = ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING:r_Source","Std. Error"],3),", t( ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING:r_Source","df"],3)," ) = ",round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING:r_Source","t value"],3), ", *p* = ",format(round(summary(GER_no_slope_inter.lmer)$coefficients["MatchMISMATCHING:r_Source","Pr(>|t|)"],3)),nsmall=3)` (see the detailed report in Appendix 4). This result suggested that<!---Traditional Chinese---> Germany study could have a robust estimation in the circumstance multiple teams conducted the study in terms of one the same protocol. Combined with the previous results <!---of Germany Traditional Chinese [@chenDoesObjectSize2020]--->[@kosterMentalSimulationObject2018], future research on Germany <!---this language--->could explore any potential linguistic aspects that might result in the match advantage of object orientation<!--- and other properties--->. <!---Although this study is unable to which aspects contributed to the resultsprovide further advice, the advantage for the future Traditional Chinese language-specific studies would have be a precise sample size justification on the participants and stimulus items.--->