

```{r load_info, message=FALSE, warning=FALSE, include=FALSE}
# Load lab information
# https://osf.io/dtfcg
lab_info <-  dir(path = "..",full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "Lab_info.csv")  %>%
  read_csv()

# https://osf.io/dtfcg
lab_fin <-dir(path = "..",full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "lab_fin.csv")  %>%
  read_csv()

# Load meta data of in site data: age, female numbers
# https://osf.io/vnbkm
insite_meta <- dir(path = "..",full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "insite_meta.csv") %>%
  read_csv() 

# Load and summary meta data of online data: age, female numbers, language proficiency
# https://osf.io/vnbkm
osweb_meta <- dir(path = "..",full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "jatos_meta.csv") %>%
  read_csv() %>%
  mutate(gender = ifelse(gender==1,"FEMALE",ifelse(gender==2,"MALE","OTHER"))) %>%
  mutate(birth_year_tr = as.numeric(birth_year)) %>%
  mutate(year = ifelse(birth_year_tr > 21 & !is.na(birth_year_tr), 1900 + birth_year_tr, 2000 + birth_year_tr)) %>%
  mutate(age = ifelse(!is.na(year),2021-year,-99)) %>%
  group_by(Batch) %>%
  summarise(N = n(), Female_N = sum(gender=="FEMALE",na.rm = TRUE), Age = mean(age, na.rm=TRUE), Proficiency = mean(lang_prof))

## count how many lab collected the data
all_lab_ID = unique(c(insite_meta$PSA_ID,osweb_meta$Batch))
lab_len = length(all_lab_ID) - 3 ## minus three additional ID for non-native participants.
```


```{r functions_pak01, message=FALSE, warning=FALSE, include=FALSE}
## Turn initial letter to upper style.
firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}
```


# Method

## Hypotheses and Design

<!---Both the sentence-picture verification task and the picture-picture verification task involve  a between-participant and a within-participant independent variable. The between-participant variable is the 18 languages registered in this study. The within-participant variable is the match or mismatch in object orientation. This binary factor, i The study design for the sentence-picture and picture-picture verification task was mixed using between-participant (language) and within-participant (match versus mismatch object orientation) independent variables. In the sentence-picture verification task, the match condition reflects the a matching between the sentence and the picture, whereas in the picture-picture verification, it reflects the a match in orientation settings between two pictures. The only dependent variable for both tasks is was the response time.
The study design for the sentence-picture and picture-picture verification task was mixed using between-participant (language) and within-participant (match versus mismatch object orientation) independent variables. In the sentence-picture verification task, the match condition reflects a matching between the sentence and the picture, whereas in the picture-picture verification, it reflects a match in orientation between two pictures. The only dependent variable for both tasks was response time. We did not select languages systematically, but instead based on our collaboration recruitment with the Psychological Science Accelerator  ---> 
The study design for the sentence-picture and picture-picture verification task was mixed using between-participant (language) and within-participant (match versus mismatch object orientation) independent variables. In the sentence-picture verification task, the match condition reflects a matching between the sentence and the picture, whereas in the picture-picture verification, it reflects a match in orientation between two pictures. The only dependent variable for both tasks was response time. The time difference between conditions in each task are the measurement of orientation effects and mental rotation scores. We did not select languages systematically, but instead based on our collaboration recruitment with the Psychological Science Accelerator [PSA, @moshontzPsychologicalScienceAccelerator2018].

<!---In the sentence-picture verification task, we expected response time to be shorter for matching compared to mismatching orientations within each language. We expect to see the match advantage within each language. We did not select languages systematically, but instead based on who our collaborators could recruit our collaboration recruitment with the Psychological Science Accelerator (PSA). We did not have any specific hypotheses about the relative size of the object orientation match advantage in different languages. In the picture-picture verification task, we expected shorter response time for identical orientation compared to different orientations. We computed an imagery score by subtracting the verification time for identical orientation from the verification time for different orientations. Based on the assumption that the mental rotation is a general cognitive aspect, we expect imagery scores to be the same on average across languages, and can be used to predict a possible match advantage [see @chenDoesObjectSize2020]. --->

(1) In the sentence-picture verification task, we expected response time to be shorter for matching compared to mismatching orientations within each language. In the picture-picture verification task, we expected shorter response time for identical orientation compared to different orientations. We did not have any specific hypotheses about the relative size of the object orientation match advantage in different languages. 

(2) We computed an imagery score by subtracting the verification time for identical orientation from the verification time for different orientations. Based on the assumption that the mental rotation is a general cognitive aspect, we expect null imagery score across languages and no association with mental simulation effects<!---match advantage---> [see @chenDoesObjectSize2020].

## Participants

<!---Through the collaboration of The Psychological Science Accelerator In collaboration with the PSA [@moshontzPsychologicalScienceAccelerator2018], we collected data in 18 languages from `r lab_len` laboratories. Our a priori power analysis based on past English studies recommended one language would have at least one thousand participants based on the current design for one language[^1]. Among the 18 languages, only English data approached this number because 17 laboratories were in the countries English is the primary language. For the rest of 17 languages, the laboratories followed the secondary plan: they collected the latest number, 50 participants, then inspected the reached effect by the Bayesian sequential analysis ( see Appendix 2 ).
Based on the preregistered plan, the available participants’ accuracy had to reach 70%. Participants were included in the final analysis if they achieved 70% accuracy based on our preregistered plan. Before the pandemic outbreak, `r format(sum(insite_meta$N), scientific=FALSE, big.mark = ",")` participants (`r format(sum(insite_meta$Female_N), scientific=FALSE, big.mark = ",")` women; *M* = `r format(round(mean(insite_meta$Age),2), scientific=FALSE, big.mark = ",")` years old) from `r length(insite_meta$PSA_ID)` laboratories joined and finished the study. After the study migrated online, there were `r sum(osweb_meta$N)` participants (`r sum(osweb_meta$Female_N)` women; *M* = `r round(mean(osweb_meta$Age,na.rm=TRUE),2)` years old) from `r length(osweb_meta$Batch)` laboratories who completed the study. Web-based participants  at the beginning  heard  the auditory instructions at the beginning of the study and had to correctly answer at least 2 of 3 comprehension check questions about the instructions. All participating laboratories had ethical approval before data collection. All data and analysis scripts are available on the source files (https://osf.io/p7avr/). Appendix 1 summarizes the average characteristics by language and laboratory.--->

The preregistered power analysis indicated n = 156 to 620 participants for 80% power for a directional one-sample t-test for a d = 0.20 and 0.10, respectively. A separate mixed-model simulation suggested that n = 400 participants with 100 items (i.e., 24 planned items nested within at least five languages) would produce 90% power to detect the same effect as Zwaan and Preacher (2012). The laboratories  were allowed to follow a secondary plan: a team collected at least their preregistered minimum sample size (suggested 100 to 160 participants, most implemented 50), and then determine whether or not to continue data collection via Bayesian sequential analysis (stopping data collection if BF10 = 10 or − 10)[^1]. 

In collaboration with the PSA, we collected data in 18 languages from 47 laboratories.  Each laboratory chose a maximal sample size and an incremental n for sequential analysis before their data collection. Because the preregistered power analysis did not match the final analysis plan, we additionally completed a sensitivity analysis to ensure sample size was adequate to detect small effects, and the results indicated that each effect could be detected at a 2.23 millisecond range for the object orientation effect. 

Before the pandemic outbreak, 2,340 participants (1,104 women; M = 21.46 years old) from 33 laboratories joined and finished the study. After the study migrated online, there were additional 4209 participants (2778 women; M = 23.75 years old) from 20 laboratories who completed the study. Web-based participants heard auditory instructions at the beginning of the study and had to correctly answer at least 2 of 3 comprehension check questions about the instructions. All participating laboratories had either ethical approval or institutional evaluation before data collection. All data and analysis scripts are available on the source files (https://osf.io/p7avr/). Appendix 1 summarizes the average characteristics by language and laboratory. 


[^1]: See details of power analysis in the preregistered plan, p. 13 ~ 15. https://psyarxiv.com/t2pjv/

## General Procedure and Materials

<!---Participating laboratories conducted the tasks as follows. --->In the beginning of the sentence-picture verification task, participants had to correctly answer all the practice trials<!--- as the instruction--->. <!---(see \autoref{fig:procedure} for an outline of the general procedure). The sentence-picture verification task will start before or after the survey of @phillsGenderedSocialCategoryinpreparation.---> Each trial started with a left-justified and horizontally centered fixation point displayed for 1000 ms, immediately followed by the probe sentence. The sentence was presented until the participant pressed the space key, acknowledging that they understood the sentence. Then, the object picture was presented in the center of the screen until the participant responded otherwise it disappeared after 2 seconds. Participants were instructed to verify the object picture mentioned in the probe sentence as quickly and accurately as they could. Following the original study [@stanfield_effect_2001], a memory check test was carried out after every three to eight trials to ensure that the participants had read each sentence carefully. 

The picture-picture verification task used the same object pictures. In each trial, two objects appeared on either side of the central fixation point until either the participant indicated that the pictures displayed the same object or two different objects or until 2 seconds elapsed. In the trials where the same object was displayed<!--- the same object--->, the pictures on each side were presented the same orientation (both were horizontal/vertical) or different orientations (one was horizontal; one was vertical).
<!---
Two pictures showing the same critical object appeared in each “yes” trial; two pictures showing two different objects from the filler items appeared in each “no” trial. 
--->

<!---All the procedures are compiled in--->The study was executed using OpenSesame software for millisecond timing <!---scripts---> [@mathotOpenSesameOpensourceGraphical2012].  Before the COVID-19 pandemic broke out, `r sum(!(insite_meta$PSA_ID %in% osweb_meta$Batch))-1` participating laboratories had completed data collection. The <!---remaining--->other 4 laboratories had to stop in person data collection because of local lockdowns. The project team decided to move data collection online. To minimize the differences between on-site and web-based studies, we converted the original Python code to Javascript and collected the data using OpenSesame through a JATOS server [@langeJustAnotherTool2015]. After the changes in the procedure were approved by the journal editor and reviewers, we proceeded with the online study from February to June 2021. For the remote version, a recorded set of verbal instructions was played at <!---first--->the beginning of the study. Participants had to confirm they were native speakers of the targeted language. All verbal briefings were packaged in the language-specific scripts. Appendix 2 describes the deployment of the scripts and the results of participants’ fluency tests. Following the literature, we did not anticipate any theoretically important differences between the two data sources [see @anwyl-irvineGorillaOurMidst2020; @bridgesTimingMegastudyComparing2020a; @deleeuwPsychophysicsWebBrowser2016]. The instructions and experimental scripts are available at the public OSF folder (https://osf.io/e428p/ "Materials" in Files). 


```{r all_data, message=FALSE, warning=FALSE, include=FALSE}
# Load raw data 
# Filter SP verification responses
# https://osf.io/xzwc4
SP_V <-  dir(path = "..",
      pattern = "all_rawdata_SP_V",   ## include in-site and internet data
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
      subset(Match != "F") %>% #only exclude fillers so we can check accuracy 
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    filter(PSA_ID != "TUR_007E" & PSA_ID != "TWN_002E") %>% ## Exclude the data of non-native participants
    mutate(PSA_ID = str_replace(PSA_ID, "SRB_002B", "SRB_002")) %>%
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id

Raw_total <- SP_V %>%
    group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
    group_by(Language, PSA_ID) %>%
    summarise(Raw_N = n())

# Load PP verification responses
# https://osf.io/ym7j3
PP <- dir(path = "..",
      pattern = "all_rawdata_PP", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
      subset(Identical != "F") %>% #only exclude filler so we can check accuracy
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id


# Load SP memory responses
# https://osf.io/zk4aj
SP_M <- dir(path = "..",
      pattern = "all_rawdata_SP_M", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv()  %>%    
      # subset(correct == 1) %>%  ## Exclude the incorrect responses and filler trials
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungrian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id
```

```{r erin_counts, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# check for participants with too many trials
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())


## site_CAN_020_1
## View(SP_V %>% filter(Subject == "site_CAN_020_1"))
SP_V$Subject[SP_V$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 24),
                                                    rep("site_CAN_020_1_2", 24))
## osweb_PSA_002_587
## View(SP_V %>% filter(Subject == "osweb_PSA_002_587"))
## nrow(SP_V)
SP_V <- SP_V %>% 
  group_by(Subject) %>% 
  filter(!duplicated(Target))
## nrow(SP_V)
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())

PP_counts <- PP %>% group_by(Subject) %>% summarize(n = n())

SP_M_counts <- SP_M %>% group_by(Subject) %>% summarize(n = n())

SP_M$Subject[SP_M$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 11), 
                                                    rep("site_CAN_020_1_2", 11))



SP_V_counts  %>% filter(n!=24)

PP_counts %>% filter(n!=24)  ## post-study survey had no labels in OSWEB script
```

```{r site_SP_V, message=FALSE, warning=FALSE, include=FALSE}
## `SP_V_subj_site` is for meta analysis at first. Now this data set is unavailable for the following analysis.

## Summarize the valid participants' SP verification data
SP_V_subj_site <- SP_V %>%
    filter(opensesame_codename!="osweb") %>%  # exclude jatos data
    group_by(Subject) %>%
    mutate(acc = sum(correct)/n()) %>%
    #mutate(acc = n()/24) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
##    summarise(V_RT = median(response_time), V_Acc = n()/12)
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n()) 

## Export for app3
## write_csv(SP_V_subj_site, file="SP_V_subj_site.csv") 
## Do not conduct app3 due to the meta-analysis updated

## Tidy SP V data for mixed linear model. Acc < .70 are included.
SP_V_site_tidy <- SP_V %>% 
  filter(Source!="osweb")
```


```{r site_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_site_tidy <- SP_M %>% 
    filter(Source != "osweb")

## Summarize the valid participants' SP memory data
SP_M_subj_site <- SP_M_site_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
#  summarise(M_Acc = n()/11)
  summarise(M_Acc = sum(correct)/n())
```



```{r site_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_site_tidy <- PP %>% 
    filter(Source!="osweb") 


## Summarize the valid participants' PP verification data
PP_subj_site <- PP_site_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(P_RT = median(response_time), P_Acc = n()/12) 
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```


```{r count_site, message=FALSE, warning=FALSE, include=FALSE}
sum_site <- (SP_V_site_tidy %>% # first part: SP V
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
    group_by(Language, PSA_ID) %>%
  summarise(SP_N = n())) %>% #divide by 2 for match/no match 
left_join(
  SP_M_subj_site %>%           # second part: memory check
    group_by(Language, PSA_ID) %>%
    summarise(M_N = n()),
  by = c("Language","PSA_ID")) %>%
left_join(  
(PP_site_tidy %>%     # third part: PP
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n())),   #divide by 2 for identical/different orientations 
by=c("Language","PSA_ID")
) 

##sum_site %>% filter((SP_N != M_N) & (SP_N != PP_N))
```


```{r online_SP_V, message=FALSE, warning=FALSE, include=FALSE}
SP_V_osweb_tidy <-  SP_V %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
    subset(Match != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP



## `SP_V_subj_osweb` is for meta analysis at first. Now this data set is unavailable for the following analysis.
SP_V_subj_osweb <- SP_V_osweb_tidy %>%
#    group_by(subject_nr) %>%
    group_by(Subject) %>%
#    mutate(acc = n()/24) %>%
    mutate(acc = sum(correct)/n()) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(V_RT = median(response_time), V_Acc = n()/12) 
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n()) 

## Export for app3
##write_csv(SP_V_subj_osweb, file="SP_V_subj_osweb.csv")
## This code is unavailable because the meta-analysis is updated.

```


```{r online_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_osweb_tidy <-  SP_M %>%
      filter(Source=="osweb") %>%   # include jatos data
      distinct() %>% ## Merge the language aspects
      filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' SP memory data
SP_M_subj_osweb <- SP_M_osweb_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
  #group_by(Language, PSA_ID, subject_nr) %>% 
  summarise(M_Acc = sum(correct)/n())
  #summarise(M_Acc = n()/11)
```


```{r online_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_osweb_tidy <-  PP %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
    subset(Identical != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' PP verification data
PP_subj_osweb <- PP_osweb_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
#    summarise(P_RT = median(response_time), P_Acc = n()/12) 
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```

```{r count_online, message=FALSE, warning=FALSE, include=FALSE}
## Compute the participants from labs and from Internet
sum_osweb <- (SP_V_osweb_tidy %>%
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(SP_N = n())) %>%
left_join(  
(PP_osweb_tidy %>% 
  group_by(Language, PSA_ID, Subject) %>%
    summarise(N = n()) %>%
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n())),
by=c("Language","PSA_ID")
) 
```


```{r preparation, message=FALSE, warning=FALSE, include=FALSE}
if(sum(names(SP_V_site_tidy) == names(SP_V_osweb_tidy)) == dim(SP_V_site_tidy)[2]){
  SP_V_tidy = bind_rows(SP_V_site_tidy, SP_V_osweb_tidy)
    chunk_msg01 <- c("All columns in SP_V matched")
} else {
  chunk_msg01 <- c("Not all columns in SP_V matched")
}

if(sum(names(PP_site_tidy) == names(PP_osweb_tidy)) == dim(PP_site_tidy)[2]){
  PP_tidy = bind_rows(PP_site_tidy, PP_osweb_tidy)
    chunk_msg02 <- c("All columns in PP matched")
} else {
  chunk_msg02 <- c("Not all columns in PP matched")
}


if(sum(names(SP_M_site_tidy) == names(SP_M_osweb_tidy)) == dim(SP_M_site_tidy)[2]){
  SP_M_tidy = bind_rows(SP_M_site_tidy, SP_M_osweb_tidy)
    chunk_msg03 <- c("All columns in SP_M matched")
} else {
  chunk_msg03 <- c("Not all columns in SP_M matched")
}
```


```{r erin_exclude_incorrects, include = FALSE, echo = FALSE}
nrow(SP_V_tidy)
SP_V_tidy <- SP_V_tidy %>% 
  filter(correct == 1)
nrow(SP_V_tidy)

nrow(PP_tidy)
PP_tidy <- PP_tidy %>% 
  filter(correct == 1)
nrow(PP_tidy)

nrow(SP_M_tidy)
SP_M_tidy <- SP_M_tidy %>% 
  filter(correct == 1)
nrow(SP_M_tidy)
```

```{r erin_outliers}
# We will implement a minimum response latency 160
# We will use a 2*MAD criterion to eliminate long response latencies
# SP_V_tidy and PP_tidy has the variable "Outlier" denoted the outlier.
SP_V_tidy <- SP_V_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time),
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 


PP_tidy <- PP_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time), 
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 


SP_M_tidy <- SP_M_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time)) 

# Integrate this into the outlier analysis table, change out for lmer criterion and say why
```


## Analysis plan


```{r SP-source-lme, message=FALSE, warning=FALSE, include=FALSE}
## analysis to decide if we had to analyze on-site and web-based respectively

## Use the tidy data through `preparation`, `erin_exclude_incorrects` and `erin_outliers`.
## Low-accuracy participants were reserved in this data set.
SP_V_tidy$r_Source = if_else(SP_V_tidy$Source == "site",1,0)


source_cor.lmer <- lmerTest::lmer(
  response_time ~ Match*r_Source + 
    (1|Subject) + 
    (r_Source|PSA_ID) + 
    (r_Source|Language), 
  control = lmerControl(optimizer = "bobyqa",
                        optCtrl = list(maxfun = 1e6)), 
  data = subset(SP_V_tidy, Outlier == FALSE))

## Results showed no difference regardless of inclusion/exclusion of low-accuracy participants.
source_cor_lmer_out <- round(summary(source_cor.lmer)$coefficients["r_Source",],3)
```

**Confirmatory Analysis** <!---According to o--->Our preregistered analysis plan[^2]  employed the fixed-effects meta-analysis model that estimated the match advantage across laboratories and languages. The meta-analysis summarized the median reaction times by match condition  then determine the effect size by laboratory. For the languages for which at least two teams collected data, we computed the meta-analytical effect size for these language data.
The mixed-effect models used each individual response time as the dependent variable and analyzed the fixed effects of matching condition using participant, target item, laboratory, and language as random intercepts (Baayen et al., 2008). All the final mixed-effects models were selected by pursuing a maximal random-effects structure whilst allowing the model to converge (Bates et al., 2015). Because of the data from the Internet after COVID outbreaked, we at first evaluated the mixed-effects model with the fixed effects of match condition and data source and the four random intercepts. This analysis showed no difference between data sources: _b_ = `r source_cor_lmer_out["Estimate"]`, _SE_ = `r source_cor_lmer_out["Std. Error"]`, _t_( `r source_cor_lmer_out["df"]` ) = `r source_cor_lmer_out["t value"]`, _p_ = `r source_cor_lmer_out["Pr(>|t|)"]`. Therefore, the following mixed-effects models did not separate on-site and the web-based data. Language-specific mixed-effect models were conducted if the meta-analysis showed the positive result. 
<!--- , this study used meta-analysis and mixed-effect models to estimate the match advantage across languages. The meta-analysis summarized the median see Stage2_Results.Rmd `meta_setup` reaction times by match condition to determine the global effect size. This approach was compatible with ANOVA used by the original study [@stanfield_effect_2001]. The mixed-effect models involved the actualused each individual response time as the dependent variable and analyzed the fixed effects of matching condition using mixed-effects models participant, target item, and lab id as random intercepts [@baayen_mixed-effects_2008]. This approach was used by recent studies [@chenDoesObjectSize2020; @kosterMentalSimulationObject2018]. Without the systematic comparison of pros and cons between the two approaches, the current analysis employed two approaches to estimate the match advantage. The statistical analyses were conducted by R packages  including *metafor* for meta analysis [@viechtbauerConductingMetaAnalysesMetafor2010], *lme4* [@batesFittingLinearMixedEffects2015] and *lmerTest* [@kuznetsovaLmerTestPackageTests2017] for mixed-effects models, as well as multiple regression through $R$ base package (Version 4.1.3; R Core Team, 2022). 

Imagery scores are were the dependent measure of the picture-picture verification responses. Tidied rResponse times were summarized by the difference between the identical and different orientation. According to our preregistered analysis plan [^3], we first evaluated the equality of imagery scores across languages in use of the mixed-effects models. Our other linear regression analysis evaluated the imagery scores as the predictor of match advantage, and In a best fit model having the imagery score as the predictor, the slope would represent the association of imagery score and match advantage. indicate its accountability. --->


[^2]: See the analysis plan in the preregistered plan, p. 19 ~ 20. https://psyarxiv.com/t2pjv/

Mental rotation scores were the dependent measure of the picture-picture verification responses. Response times were summarized by the difference between the identical and different orientation. According to our preregistered analysis plan[^3], we first evaluated the equality of imagery scores across languages in use of ANOVA. Because the later data collection was on the Internet, we used mixed models instead of ANOVA to evaluate the difference of data sources. The other planned analysis was the linear regression analysis in use of imagery scores as the predictor of match advantage. We evaluated the necessity of this analysis according to the outcomes of mixed-effect models.

[^3]: See the analysis plan in the preregistered plan, p. 21. https://psyarxiv.com/t2pjv/


<!---**Exploratory Analysis**. We conducted mixed-effect models for languages that reached the recommended sample size from our power analyses and for languages that showed a significant match advantage. In one of the cases below we conducted the mixed-effect models for some language dataset. At first the total sample size reached recommended sample size as our prior power analysis. Otherwise the meta-analysis indicated a language dataset showed a significant match advantage. Although this analysis was not in the preregistered analysis plan, the authors contributed to the methodology agreed this analysis could improve the reliability of the linguistic-specific result.  --->

**Decision criterion.** <!---P---> _p_-values were interpreted using the preregistered alpha level of .05. <!---Because in our preregistered plan each language was assumed a standalone group, _p_-values  of the analysis by each language were not corrected [@armstrongWhenUseBonferroni2014]. All the final mixed-effects models were selected by pursuing a maximal random-effects structure whilst allowing the model to converge [@batesFittingLinearMixedEffects2015].---> _p_-values for each effect were calculated using the Satterthwaite approximation for degrees of freedom [@lukeEvaluatingSignificanceLinear2017].


<!---
```{r procedure, eval=FALSE, fig.cap="Procedures", message=FALSE, warning=FALSE, include=FALSE}
##knitr::include_graphics(c("includes/fig/fig02.png"))
```
--->



```{r ktable-power, eval=FALSE, fig.width=150, message=FALSE, warning=FALSE, include=FALSE, results='asis'}
require(kableExtra)
##require(dplyr)

est_power <- data.frame(np = rep(c(200,400,800,1200), each=3),
                        ni = rep(c(24,48,100),4),
                        PANGEA = c(.136,.208,.311,.151,.247,.401,.161,.274,.471,.165,.285,.500),
                        simr = c(.326,.525,.736,.412,.647,.901,.469,.748,.943,.480,.779,.965)) 

colnames(est_power) <- c("Number of Participants","Number of Items","PANGEA","simr")

est_power %>% kable(align = "c", caption = "Achieved power estimated by PANGEA and simr.", booktabs = TRUE) %>%
  kable_styling(latex_options = c
("hold_position"), position = "center") %>%
  add_header_above(c(" " = 2, "Estimated Power" = 2)) %>%
  footnote(general = "The settings of PANGEA is available at https://osf.io/mxnrb/",
           general_title = "Note. ",
           footnote_as_chunk = T, title_format = c("italic")) %>%
  collapse_rows(columns = 1, valign = "top")
```


```{r map, eval=FALSE, fig.cap="Distribution", message=FALSE, warning=FALSE, include=FALSE, out.width="100%"}
knitr::include_graphics(c("includes/fig/map.png"))
```


```{r lab-info, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results='asis'}

tbl_note <- paste("Recruitment of the participating laboratories is going on while we submitted this proposal. Last updated:", format(Sys.Date(), "%Y/%m/%d"))

read.csv("Lab-summary.csv") %>%
  apa_table(caption = "Summary of registered participating laboratories in the function of primary languages and sample size.",
            col.names = c("Language","Number of Laboratories","Registered Sample Size"),
            note = tbl_note,
            escape = TRUE)
```

