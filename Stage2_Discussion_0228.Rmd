

# Discussion

After estimating the match advantage of object orientation across 18 languages, no evidence for a global effect was found, but the meta-analysis and mixed-effect models indicated the marginal match advantage was present in the investigated languages that had at least two datasets. Traditional Chinese, especially, showed marginal results which are consistent with the findings of @chenDoesObjectSize2020. This suggests that the match advantage of object orientation for many languages are small. Thus large sample sizes are needed to determine if the match advantage for a language is significantly different from zero. One exception might be a language has some unique features that amplify the match advantage. This requirement is especially onerous in cross-linguistic studies when it is difficult to reach the desirable sample size. In sum, the present results put into question the robustness of cross-linguistic studies.

The second question addressed whether the mental simulation of object orientation could be predicted by mental rotation, which was operationalized as imagery scores. The mixed-effects models indicated that imagery scores were hardly affected by the different languages and data collection procedures. Regarding the planned regression analysis, the imagery scores underpredicted the effect of orientation match. In conclusion, the current findings barely confirm the predictions based on the mental simulation theory.

## Measurement issues across platforms

The precision of web-based experiments has been previously investigated [@anwyl-irvineGorillaOurMidst2020; @bridgesTimingMegastudyComparing2020a]. In the present study, the responses to the sentence-picture verification task collected on the web (*Mdn* = `r format(round(median(SP_V_osweb_lme_data$response_time)), scientific=FALSE, big.mark = ",")`, *SD* = `r format(round(sd(SP_V_osweb_lme_data$response_time)), scientific=FALSE, big.mark = ",")`) were roughly twice as long as those collected in labs (*M* = `r round(mean(SP_V_osweb_lme_data$response_time))`, *SD* = `r round(sd(SP_V_osweb_lme_data$response_time))`). Previous studies have also found online responses to be longer than on-site ones, but the current difference is larger. For instance, @deleeuwPsychophysicsWebBrowser2016 collected response times of just under 100 ms, and found that online responses were 10--40 ms longer than on-site ones. Our primary concern was whether OpenSesame could have caused a higher measurement error in the on-site data than in the web-based data. Of the frequently used desktop applications, OpenSesame Windows version has the highest precision and relatively low variation [see Table 2 in @bridgesTimingMegastudyComparing2020a]. Although Bridges et al. did not evaluate the performance of OSWeb, PsychoPy [@peircePsychoPy2ExperimentsBehavior2019], which is the basis for OpenSesame, had higher precision than OpenSesame desktop version. Specifically, it had a 25 to 50 ms lag [see Table 3 of @bridgesTimingMegastudyComparing2020a] in many combinations of operating systems and web browsers. This lag is shorter than the response time difference between the data sources, suggesting that measurement precision was not the source of the timing discrepancies between on-site and web-based data. 


According to our meta-analysis, the data from `r sum(SP_V_meta_all$yi>0)` teams revealed marginal match advantages: `r sum(SP_V_meta_site$yi>0)` of these teams collected data on site (e.g., NOR 003) and `r sum(SP_V_meta_osweb$yi>0)` collected data online (e.g., NZL 005). The former example of the online data is from a lab testing in Norwegian, a language in which the match advantage of orientation had not been studied before, to our knowledge. The latter example is from a lab testing in English, a language that has yielded marginal match advantages of object orientation before [@chenDoesObjectSize2020; @stanfield_effect_2001; @zwaanRevisitingMentalSimulation2012]. 

## Generalizability and Limitations  

To acknowledge deviations from the preregistration, it must be noted that the final data included four more languages than were initially planned, and that some data were collected on the web due to the Covid-19 pandemic, instead of in labs as planned. We do not know of any research suggesting how these deviations could have affected our results. For instance, we reviewed research suggesting that on-site and web-based data need not substantially differ [e.g., @deleeuwPsychophysicsWebBrowser2016]. 

This study reflected the difficulty of investigating cognition across languages, especially when dealing with effects that require large sample sizes [see @lokenMeasurementErrorReplication2017; @vadilloUnderpoweredSamplesFalse2016]. Indeed, a fundamental challenge for our project was substantial variation in the number of participants available for the languages we investigated. It must also be noted that the mixed-effects models could have been more conservative by prioritizing the maximal random-effects structure over the achievement of model convergence [@brauerLinearMixedeffectsModels2018].

Some languages in this study had one participating team only or did not have sufficient data for the exploratory analysis: Arabic, Brazilian Portuguese, European Portuguese, Greek, Hebrew, Hindi, Hungarian, Polish, Serbian, Simplified Chinese and Thai. As a consequence, we do not know if these languages would show a robust match advantage if they were studied by more than two laboratories plus a sample size as large as that which we had for English. Researchers could use stimuli employed for the present study to launch new studies that focus on these specific languages. With rigorous power analyses and large multi-site collaborative projects, future research on specific languages has the potential to provide robust estimates of match advantages associated with a variety of object properties. Such research could serve as a firm foundation for developing a strong theory of mental simulation.   

