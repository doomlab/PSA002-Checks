

```{r all_data, message=FALSE, warning=FALSE, include=FALSE}
# Load raw data 
# Filter SP verification responses
SP_V <-  dir(path = "..",
      pattern = "all_rawdata_SP_V",   ## include in-site and internet data
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
      subset(Match != "F") %>% #only exclude fillers so we can check accuracy 
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungarian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id


# Load PP verification responses
PP <- dir(path = "..",
      pattern = "all_rawdata_PP", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv() %>%
      # subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
      subset(Identical != "F") %>% #only exclude filler so we can check accuracy
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungarian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id

# Load SP memory responses
SP_M <- dir(path = "..",
      pattern = "all_rawdata_SP_M", 
      recursive = TRUE, full.names = TRUE) %>% 
      read_csv()  %>%    
      # subset(correct == 1) %>%  ## Exclude the incorrect responses and filler trials
      inner_join(select(lab_info, PSA_ID, Language), by = "PSA_ID") %>%
    distinct() %>% ## Merge the language aspects
    mutate(Language = ifelse(Language == "Magyar", "Hungarian", Language)) %>%  ## Switch "Magyar" to "Hungarian"
    mutate(Language = ifelse(Language == "Simple Chinese", "Simplified Chinese", Language)) %>%  ## Switch "Simple Chinese" to "Simplified Chinese"
    mutate(Source = if_else(opensesame_codename == "osweb","osweb","site"), 
           Subject = paste0(Source,"_",PSA_ID,"_",subject_nr)) ## Compose the unique participant id
```

```{r erin_counts}
# check for participants with too many trials
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())

## site_CAN_020_1
## View(SP_V %>% filter(Subject == "site_CAN_020_1"))
SP_V$Subject[SP_V$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 24),
                                                    rep("site_CAN_020_1_2", 24))
## osweb_PSA_002_587
## View(SP_V %>% filter(Subject == "osweb_PSA_002_587"))
## nrow(SP_V)
SP_V <- SP_V %>% 
  group_by(Subject) %>% 
  filter(!duplicated(Target))
## nrow(SP_V)
SP_V_counts <- SP_V %>% group_by(Subject) %>% summarize(n = n())

PP_counts <- PP %>% group_by(Subject) %>% summarize(n = n())

SP_M_counts <- SP_M %>% group_by(Subject) %>% summarize(n = n())

SP_M$Subject[SP_M$Subject == "site_CAN_020_1"] <- c(rep("site_CAN_020_1", 11), 
                                                    rep("site_CAN_020_1_2", 11))
```

```{r site_SP_V, message=FALSE, warning=FALSE, include=FALSE}
## Exclude the participants who had a accuracy lower than the preregistered exclusion criterion (70%)

## Summarize the valid participants' SP verification data
SP_V_subj_site <- SP_V %>%
    filter(opensesame_codename!="osweb") %>%  # exclude jatos data
    group_by(Subject) %>%
    mutate(acc = sum(correct)/n()) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n()) 

## Tidy SP V data for mixed linear model
SP_V_site_tidy <- SP_V %>% 
  filter(Source!="osweb")
```

```{r site_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_site_tidy <- SP_M %>% 
    filter(Source != "osweb")

## Summarize the valid participants' SP memory data
SP_M_subj_site <- SP_M_site_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
  summarise(M_Acc = sum(correct)/n())
```

```{r site_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_site_tidy <- PP %>% 
    filter(Source!="osweb") 

## Summarize the valid participants' PP verification data
PP_subj_site <- PP_site_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```

```{r count_site, message=FALSE, warning=FALSE, include=FALSE}
sum_site <- (SP_V_subj_site %>% 
  group_by(Language, PSA_ID) %>%
  summarise(SP_N = n()/2)) %>% #divide by 2 for match/no match 
left_join(  
(PP_subj_site %>% 
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n()/2)),
by=c("Language","PSA_ID")
) 
```

```{r online_SP_V, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP V data for mixed linear model
SP_V_osweb_tidy <-  SP_V %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Match != "F") %>%  ## Exclude the incorrect responses and filler trials
    subset(Match != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' SP verification data
SP_V_subj_osweb <- SP_V_osweb_tidy %>%
    group_by(Subject) %>%
    mutate(acc = sum(correct)/n()) %>%
    filter(acc > .7) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
    summarise(V_RT = median(response_time), V_Acc = sum(correct)/n()) 
```

```{r online_SP_M, message=FALSE, warning=FALSE, include=FALSE}
## Tidy SP M data for mixed linear model
SP_M_osweb_tidy <-  SP_M %>%
      filter(Source=="osweb") %>%   # include jatos data
      distinct() %>% ## Merge the language aspects
      filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' SP memory data
SP_M_subj_osweb <- SP_M_osweb_tidy %>%
  group_by(Language, PSA_ID, Subject) %>% 
  summarise(M_Acc = sum(correct)/n())
```

```{r online_PP, message=FALSE, warning=FALSE, include=FALSE}
## Tidy PP data for mixed linear model
PP_osweb_tidy <-  PP %>%
      filter(Source=="osweb") %>%   # include jatos data
      #subset(correct == 1 & Identical != "F")  %>%  ## Exclude the incorrect responses and filler trials
    subset(Identical != "F") %>% 
    distinct() %>% ## Merge the language aspects
    filter(!(PSA_ID == "USA_033" & subject_nr == 39)) ## exclude this participant who had not complete PP

## Summarize the valid participants' PP verification data
PP_subj_osweb <- PP_osweb_tidy %>%
    mutate(Match = (Orientation1 == Orientation2)) %>%
    group_by(Language, PSA_ID, Subject, Match) %>%
    summarise(P_RT = median(response_time), P_Acc = sum(correct)/n()) 
```

```{r functions_pak02, message=FALSE, warning=FALSE, include=FALSE}
## Compute the participants from labs and from Internet
sum_osweb <- (SP_V_subj_osweb %>% 
  group_by(Language, PSA_ID) %>%
  summarise(SP_N = n()/2)) %>%
left_join(  
(PP_subj_osweb %>% 
  group_by(Language, PSA_ID) %>%
  summarise(PP_N = n()/2)),
by=c("Language","PSA_ID")
) 

firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}
```

# Results

Within the data collected on-site, `r format(sum(sum_site$SP_N), scientific=FALSE, big.mark = ",")` participants finished the sentence-picture verification task and met the preregistered inclusion criterion (accuracy percentile > 70%); `r format(sum(sum_site$PP_N), scientific=FALSE, big.mark = ",")` participants finished the picture-picture verification task. Raw data files containing data for `r xfun::numbers_to_words(sum(sum_site$PP_N) - sum(sum_site$SP_N))` participants were lost due to human error. Within the data sets collected online, `r format(sum(sum_osweb$SP_N), scientific=FALSE, big.mark = ",")` participants finished the sentence-picture verification task and met the preregistered inclusion criterion; `r format(sum(sum_osweb$PP_N), scientific=FALSE, big.mark = ",")` participants finished the picture-picture verification task. All data and analyses are available on the source files (https://osf.io/p7avr/).  

## Confirmatory analysis: Intra-lab analysis during data collection

Before data collection, each lab decided whether they wanted to apply a sequential analysis [@schonbrodtSequentialHypothesisTesting2017] or whether they wanted to settle for a fixed sample size. The preregistered protocol for labs applying sequential analysis established that they could stop data collection upon reaching the preregistered criterion ($BF_{10} = 10\ or\ -10$), or the maximal sample size. Most laboratories either chose a fixed sample size without applying sequential analysis, or applied sequential analysis and reached their maximal sample size. 

Two laboratories (HUN 001, TWN 001) stopped data collection at the preregistered criterion. Some laboratories did not conduct the sequential analysis on all their data because of one of the following reasons: (1) their data collection was interrupted by the pandemic outbreak; (2) participants performed worse in the online study; (3) too many of their participants were non-native speakers. Lab-specific results were reported on a public website as each laboratory completed data collection (details available in Appendix 2).

## Confirmatory analysis: Inter-lab analysis of final data

```{r preparation, message=FALSE, warning=FALSE, include=FALSE}
if(sum(names(SP_V_site_tidy) == names(SP_V_osweb_tidy)) == dim(SP_V_site_tidy)[2]){
  SP_V_tidy = bind_rows(SP_V_site_tidy, SP_V_osweb_tidy)
    chunk_msg01 <- c("All columns in SP_V matched")
} else {
  chunk_msg01 <- c("Not all columns in SP_V matched")
}

if(sum(names(PP_site_tidy) == names(PP_osweb_tidy)) == dim(PP_site_tidy)[2]){
  PP_tidy = bind_rows(PP_site_tidy, PP_osweb_tidy)
    chunk_msg02 <- c("All columns in PP matched")
} else {
  chunk_msg02 <- c("Not all columns in PP matched")
}

if(sum(names(SP_M_site_tidy) == names(SP_M_osweb_tidy)) == dim(SP_M_site_tidy)[2]){
  SP_M_tidy = bind_rows(SP_M_site_tidy, SP_M_osweb_tidy)
    chunk_msg03 <- c("All columns in SP_M matched")
} else {
  chunk_msg03 <- c("Not all columns in SP_M matched")
}
```

```{r erin_exclude_incorrects, include = FALSE, echo = FALSE}
nrow(SP_V_tidy)
SP_V_tidy <- SP_V_tidy %>% 
  filter(correct == 1)
nrow(SP_V_tidy)

nrow(PP_tidy)
PP_tidy <- PP_tidy %>% 
  filter(correct == 1)
nrow(PP_tidy)

nrow(SP_M_tidy)
SP_M_tidy <- SP_M_tidy %>% 
  filter(correct == 1)
nrow(SP_M_tidy)
```

```{r erin_outliers}
# We will implement a minimum response latency 160
# We will use a 2*MAD criterion to eliminate long response latencies
SP_V_tidy <- SP_V_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time),
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 

PP_tidy <- PP_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time), 
         Outlier = response_time <= 160 | response_time >= (med + 2*MAD)) 

SP_M_tidy <- SP_M_tidy %>% 
  group_by(Subject) %>% 
  mutate(MAD = mad(response_time),
         med = median(response_time)) 

# Integrate this into the outlier analysis table, change out for lmer criterion and say why
```

```{r cycle_outlier_check, message=FALSE, warning=FALSE, include=FALSE}

# get_intercept <- function(set) {
#   t <- cbind(
#              Subject = levels(as.factor(set$Subject)),
#              (lmer(response_time ~ Match + (1|Subject), data = set) %>%
#   coef())$Subject %>%
#       as_tibble()%>%
#   mutate(Outlier = `(Intercept)` > quantile(`(Intercept)`,probs=.75))
#     )
#   
#   return(t)
# }

outliers_table <- SP_V_tidy %>% 
    group_by(PSA_ID) %>% 
    summarize(total_n = length(unique(Subject)), 
              total_data = n(), 
              total_outliers = sum(Outlier == T), 
              prop = round(total_outliers / total_data, 2))



write_csv(outliers_table, file = "includes/files/outliers_table.csv")
```

```{r summary-site, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
# note that V_RT is before outliers and percent correct, subject_M is after both 
SP_V_tidy %>% 
  filter(Outlier == FALSE & Source == "site") %>%
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time)) %>% 
  left_join(SP_V_subj_site, by = c("Subject" = "Subject", 
                                   "Language" = "Language", 
                                   "Match" = "Match")) %>% 
  filter(!is.na(V_Acc)) %>% #exclude people who didn't mean accuracy so they are NA here 
  group_by(Language, Match) %>%
  summarise(N = n(), med_RT = median(subject_M), mean_ACC = mean(V_Acc)) %>%
  pivot_wider(names_from = Match, values_from = c(med_RT, mean_ACC)) %>%
  mutate(Effect = (med_RT_N - med_RT_Y) ) %>%
  transmute(N=N,
             Mismatch_stat = paste0(round(med_RT_N),"(",format(round(mean_ACC_N*100,digits=2),nsmall=2),")"),
             Match_stat = paste0(round(med_RT_Y),"(",format(round(mean_ACC_Y*100,digits=2),nsmall=2),")"),
             Effect = Effect) %>%
  kable(  
    format = "latex",
    booktabs = TRUE,
    escape=FALSE,
    col.names = c("Language","N","Mismatching","Matching","Match Advantage"),
    align = c("l","r","r","r","r"),
   caption = "Median reaction times and accuracy percentages (in parentheses) per match condition (Mismatching, Matching); Match advantage (difference in response times) by language in the on-site data."
    )
#  flextable() %>% 
#  set_header_labels(Language = "Language", N = "N", Mismatch_stat ="Mismatching", Match_stat = "Matching", Effect = "match advantage")
```

```{r summary-osweb, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
SP_V_tidy %>% 
  filter(Outlier == FALSE & Source == "osweb") %>%
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time)) %>% 
  left_join(SP_V_subj_osweb, by = c("Subject" = "Subject", 
                                   "Language" = "Language", 
                                   "Match" = "Match")) %>% 
  filter(!is.na(V_Acc)) %>% #exclude people who didn't mean accuracy so they are NA here 
  group_by(Language, Match) %>%
  summarise(N = n(), med_RT = median(subject_M), mean_ACC = mean(V_Acc)) %>%
  pivot_wider(names_from = Match, values_from = c(med_RT, mean_ACC)) %>%
  mutate(Effect = (med_RT_N - med_RT_Y) ) %>%
  transmute( N=N,
             Mismatch_stat = paste0(round(med_RT_N),"(",format(round(mean_ACC_N*100,2),nsmall=2),")"),
             Match_stat = paste0(round(med_RT_Y),"(",format(round(mean_ACC_Y*100,2),nsmall=2),")"),
             Effect = Effect) %>%
  kable(  
    format = "latex",
    booktabs = TRUE,
    escape=FALSE,
    col.names = c("Language","N","Mismatching","Matching","Match Advantage"),
    align = c("l","r","r","r","r"),
   caption = "Median reaction times and accuracy percentages (in parentheses) per match condition (Mismatching, Matching); Match advantage (difference in response times) by language in the web-based data."
    )
#  flextable() %>% 
#  set_header_labels(Language = "Language", N = "N", Mismatch_stat ="Mismatching", Match_stat = "Matching", Effect = "match advantage")
```

**Identification of outliers.** For each laboratory, outliers were identified by the third quantile of the grand intercept in the simplest mixed-effects model. This mixed-effects model contained the response times as the dependent measure, matching condition as the only fixed effect, and the participant as the only random intercept. Among the data sets showing outliers, the averaged proportion of outliers was 0.25. Table S4 in Appendix 1 illustrates the distribution of outliers by laboratory. Table \@ref(tab:summary-site) and Table \@ref(tab:summary-osweb) respectively summarise the match advantages by language. All the below data analysis depended on the datasets excluding the outliers. 

(Insert Table \@ref(tab:summary-site) about here )

(Insert Table \@ref(tab:summary-osweb) about here )


```{r meta_setup, message=FALSE, warning=FALSE, include=FALSE}
## Prepare the data sets for the meta analysis
## All the sources
SP_V_meta_data <- SP_V_tidy %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  group_by(Language, PSA_ID, Subject, Match) %>%
  #### This was mean before did you mean median??? #### 
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>% 
    pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5)
  

## Only from site
SP_V_site_meta_data <- SP_V_tidy %>% 
  filter(Source == "site") %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  group_by(Language, PSA_ID, Subject, Match) %>%
  #### This was mean before did you mean median??? #### 
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>% 
    pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5)

## Only from osweb
SP_V_osweb_meta_data <- SP_V_tidy %>% 
  filter(Source == "osweb") %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  group_by(Language, PSA_ID, Subject, Match) %>%
  #### This was mean before did you mean median??? #### 
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>% 
    pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5)

SP_V_combine_meta_data <- bind_rows( bind_cols(Source = "site", SP_V_site_meta_data), bind_cols(Source="osweb",SP_V_osweb_meta_data))
```


**Meta-analysis of match advantages across laboratories.** Because the preregistered analysis plan did not consider the data collected online, we conducted the overall meta-analyses for all the datasets combined data sources. In this analysis, we computed the effect size by data set and estimated the global effect size. Since data from small samples may contribute to a biased estimate, nine datasets with sample sizes smaller than 25 were excluded from the analyses. The overall meta-analysis found no match advantage (Figure \@ref(fig:meta-all)).Among the languages that had at least two datasets, we conducted the meta-analysis for English, German, Norway, Traditional Chinese, Slovak, and Turkey. Only Traditional Chinese showed a significant meta-analytic effect across laboratories(see Figure \@ref(fig:meta-tc)). Results of the other languages are available in Appendix 3.



```{r meta-all, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE, fig.cap="Meta-analysis on match advantage of object orienation for all datasets"}
es_data <- SP_V_meta_data %>%
  filter((ni > 25)) ## Exclude small sample size
SP_V_en_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=es_data)
SP_V_meta_all <-  rma.uni(yi, vi, data = SP_V_en_es, slab = PSA_ID, method = "REML", digits = 2)

forest(SP_V_meta_all, mlab = "")
```


(Insert Figure \@ref(fig:meta-all) about here)



```{r meta-tc, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Meta-analysis on match advantage of object orienation for Traditional Chinese datasets."}
## Locate English data for meta-analysis
SP_V_tc_meta_data <-SP_V_tidy %>% 
  filter(Language == "Traditional Chinese") %>%
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  group_by(Language, PSA_ID, Subject, Match) %>%
  #### This was mean before did you mean median??? #### 
  summarize(RT = median(response_time), ACC = mean(V_Acc)) %>% 
    pivot_wider(
#    cols = Match:ACC,
    names_from = Match,
    values_from = c(RT,ACC)
  ) %>%
  group_by(Language,PSA_ID) %>%
  summarise(m_match=median(RT_Y),m_mismatch=median(RT_N),
            sd_match=sd(RT_Y),sd_mismatch=sd(RT_N),
            acc_match=mean(ACC_Y),acc_mismatch=mean(ACC_N),
            ni=n()) %>%
  bind_cols(ri=.5) %>% 
  filter((ni > 25)) ## Exclude lab less than 25 participants

## Prepare the elements for computing the meta-analytic effect size
SP_V_tc_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=SP_V_tc_meta_data)

## Compute the meta-analytic effect size
SP_V_meta <-  rma.uni(yi, vi, data = SP_V_tc_es, slab = PSA_ID, method = "REML", digits = 2)

## Output the forest plot
forest(SP_V_meta, mlab = "")
```



(Insert Figure \@ref(fig:meta-tc) about here)




```{r meta-site, fig.cap="Meta-analysis", message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
## We removed this chunck after the collaborators' appendix.
es_data <- SP_V_site_meta_data %>%
  filter((ni > 25)) ## Exclude small sample size
SP_V_site_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=es_data)
SP_V_meta_site <-  rma.uni(yi, vi, data = SP_V_site_es, slab = PSA_ID, method = "REML", digits = 2)

##forest(SP_V_meta, mlab = "Data sets from sites")
```

<!---
The meta-analysis of the lab-based data showed a match advantage with small effect size and little variation among laboratories. Only one laboratory (HUN 001) found a significant match advantage (Figure \@ref(fig:meta-site)).

(Insert Figure \@ref(fig:meta-site) about here)
--->

```{r meta-osweb, fig.cap="Meta-analysis", message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
## We removed this chunck after the collaborators' appendix.
es_data <- SP_V_osweb_meta_data %>%
  filter((ni > 25)) ## Exclude small sample size
SP_V_osweb_es <- escalc(measure = "MC", 
         m1i = m_mismatch, m2i = m_match, 
         sd1i = sd_mismatch, sd2i = sd_match, 
         ni = ni, ri = ri, 
         slab = PSA_ID, data=es_data)
SP_V_meta_osweb <-  rma.uni(yi, vi, data = SP_V_osweb_es, slab = PSA_ID, method = "REML", digits = 2)

##forest(SP_V_meta, mlab = "Data sets from Internet")
```

<!---
The meta-analysis of the Internet-based data revealed a match disadvantage with small effect size. Only one laboratory (NZL 005) found a match advantage (Figure \@ref(fig:meta-osweb)). There was greater variation among lab-based datasets than the Internet-based datasets.  

(Insert Figure \@ref(fig:meta-osweb) about here)
--->


```{r SP-lme-data, message=FALSE, warning=FALSE, include=FALSE}
# export data file for the making of appendix
# SP_V_lme_data <- dir(path = "..",
#       pattern = "SP_V_lme_data.csv",   ## include in-site and internet data
#       recursive = TRUE, full.names = TRUE) %>% 
#       read_csv() %>%
#   inner_join(subset(outliers_table, Outlier == FALSE),by = c("Subject","PSA_ID" = "LAB"))
# 
# SP_V_lme_data$Language <- ifelse(SP_V_lme_data$Language == "Magyar","Hungrian",SP_V_lme_data$Language)
# not sure why this is being used since it's the same thing as SP_V_tidy? The numbers of lines
# are slightly different but seems problematic to put in random data that's 
# not clear how it's made 

SP_V_lme_data <- SP_V_tidy %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  mutate(Match = factor(Match,
                        levels = c("Y","N"),
                        labels = c("MATCHING","MISMATCHING")),
                        Source = factor(Source, 
                        levels = c("site","osweb"),
                        labels = c("Site","Internet") ))
```

```{r SP-source-lme, message=FALSE, warning=FALSE, include=FALSE}
## analysis to decide if we had to analyze on-site and web-based respectively

SP_V_lme_data$r_Source = if_else(SP_V_lme_data$Source == "Site",1,0)

source_cor.lmer <- lmerTest::lmer(
  response_time ~ Match*r_Source + 
    (1|Subject) + 
    (r_Source|PSA_ID) + 
    (r_Source|Language), 
  control = lmerControl(optimizer = "bobyqa",
                        optCtrl = list(maxfun = 1e6)), 
  data = SP_V_lme_data)

source_cor_lmer_out <- round(summary(source_cor.lmer)$coefficients["r_Source",],3)
```

```{r SP-sensitive, message=FALSE, warning=FALSE, include=FALSE}
coef_all <- as.data.frame(summary(source_cor.lmer)$coefficients)
coef_all$sig_t <- qt(.025, coef_all$df, lower.tail = F)
coef_all$min_effect <- coef_all$sig_t * coef_all$`Std. Error`
```

```{r SP_lme_update, message = FALSE, warning = FALSE}
overall_excluded_lang <- SP_V_lme_data %>%
  group_by(Language, Subject) %>%
  summarise(N_trials = n()) %>%
  group_by(Language) %>%
  summarise(N = n()) %>%
  filter(N < 25) %>%
  pull(Language)  ## Exclude the languages less than 25 participants

# just in case but doesn't appear to be excluding anyone 
SP_V_lme_data_excluded <- subset(SP_V_lme_data, !(Language %in% overall_excluded_lang))

intercept.model <- lm(response_time ~ 1, 
                      data = SP_V_lme_data_excluded)

subject.model <- lmer(response_time ~ 1 + (1|Subject), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data_excluded)

item.model <- lmer(response_time ~ 1 + (1|Subject) + (1|Target), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data_excluded)

language.model <- lmer(response_time ~ 1 + (1|Subject) + (1|Target) + (1|Language), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data_excluded)

# which is best 
AIC(intercept.model)
AIC(subject.model)
AIC(item.model)
AIC(language.model)

AIC(subject.model) < AIC(intercept.model)
AIC(item.model) < AIC(subject.model)
AIC(language.model) < AIC(item.model)

fixed.model <- lmer(response_time ~ Match + (1|Subject) + (1|Target) + (1|Language), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data_excluded)

AIC(fixed.model)
AIC(fixed.model) < AIC(language.model)

summary(fixed.model)

fixed.randomslope.model <- lmer(response_time ~ Match + (1|Subject) + (1|Target) + (1 + Match|Language), 
                      control = lmerControl(optimizer = "bobyqa",
                                            optCtrl = list(maxfun = 1e6)), 
                      data = SP_V_lme_data_excluded)

AIC(fixed.randomslope.model)
AIC(fixed.randomslope.model) < AIC(fixed.model)

summary(fixed.randomslope.model)

# plot of the data for just raw score differences
ggplot(SP_V_lme_data_excluded, aes(Language, response_time, color = Match)) + 
  theme_classic() + 
  ylab("Response Latencies") + 
  xlab("Language") + 
  stat_summary(fun = mean, 
               geom = "point") + 
  stat_summary(fun.data = mean_cl_normal, 
               geom = "pointrange") + 
  theme(axis.text.x = element_text(angle = 90))

# plot of data controlling for other factors
coef_model <- coef(fixed.randomslope.model)$Language
coef_model$MATCHING <- coef_model$`(Intercept)`
coef_model$MISMATCHING <- coef_model$MATCHING + coef_model$MatchMISMATCHING
coef_model$Language <- rownames(coef_model)

library(parameters)
se_model <- as.data.frame(standard_error(fixed.randomslope.model, effects = "random")$Language)
coef_model$se <- se_model$`(Intercept)`

coef_data <- coef_model %>% 
  pivot_longer(cols = c("MATCHING", "MISMATCHING")) %>% 
  mutate(lower = value - 2*se, 
         upper = value + 2*se) %>% 
  rename(Match = name)

ggplot(coef_data, aes(Language, value, color = Match)) + 
  theme_classic() + 
  ylab("Response Latencies") + 
  xlab("Language") + 
  geom_point() + 
  geom_pointrange(data = coef_data, aes(ymin = lower, ymax = upper)) +
  theme(axis.text.x = element_text(angle = 90))
```



## not using because don't need the split 

```{r SP-lang-lme, message=FALSE, warning=FALSE}
## Check sample size of a language by site data
site_excluded_lang <- subset(SP_V_lme_data, Source=="Site") %>%
  group_by(Language, Subject) %>%
  summarise(N_trials = n()) %>%
  group_by(Language) %>%
  summarise(N = n()) %>%
  filter(N < 25) %>%
  pull(Language)  ## Exclude the languages less than 25 participants

## Allocate the site data
SP_V_site_lme_data = subset(SP_V_lme_data, Source=="Site" & !(Language %in% site_excluded_lang))

## Run the mixed effect model by site data
site_cor.lmer =lmer(response_time ~ Language*Match + (1|Subject), 
  control = lmerControl(optimizer = "bobyqa",
                        optCtrl = list(maxfun = 1e6)), 
  data = SP_V_site_lme_data)

## compute CI of coefficients
###confint(site_cor.lmer)

## Check sample size of a language by osweb data
osweb_excluded_lang <- subset(SP_V_lme_data, Source=="Internet") %>%
  group_by(Language, Subject) %>%
  summarise(N_trials = n()) %>%
  group_by(Language) %>%
  summarise(N = n()) %>%
  filter(N < 25) %>%
  pull(Language)  ## Exclude the languages less than 25 participants

## Allocate the osweb data
SP_V_osweb_lme_data = subset(SP_V_lme_data, Source=="Internet" & !(Language %in% osweb_excluded_lang))

osweb_cor.lmer = lmerTest::lmer(response_time ~ Language*Match + (1|Subject), 
  control = lmerControl(optimizer = "bobyqa",
                        optCtrl = list(maxfun = 1e6)), # Increase maximum number of iterations to facilitate model convergence , 
                    data = SP_V_osweb_lme_data)

## Create the stat info in the article
site_cor_lme_out <- round(summary(site_cor.lmer)$coefficients["MatchMISMATCHING",],3)
site_cor_lme_greek_out <- round(summary(site_cor.lmer)$coefficients["LanguageGreek:MatchMISMATCHING",],3)
osweb_cor_lme_out <- round(summary(osweb_cor.lmer)$coefficients["MatchMISMATCHING",],3)
osweb_cor_lme_serbian_out <- round(summary(osweb_cor.lmer)$coefficients["LanguageSerbian:MatchMISMATCHING",],3)
```

```{r SP-sensitive-separate, message=FALSE, warning=FALSE, include=FALSE}
coef_all_site <- as.data.frame(summary(site_cor.lmer)$coefficients)
coef_all_site$sig_t <- qt(.025, coef_all_site$df, lower.tail = F)
coef_all_site$min_effect <- coef_all_site$sig_t * coef_all_site$`Std. Error`

#mean(coef_all_site[16:28 , "min_effect"])

coef_all_web <- as.data.frame(summary(osweb_cor.lmer)$coefficients)
coef_all_web$sig_t <- qt(.025, coef_all_web$df, lower.tail = F)
coef_all_web$min_effect <- coef_all_web$sig_t * coef_all_web$`Std. Error`

#mean(coef_all_web[10:16 , "min_effect"])
```

**Evaluating match advantages using linear mixed-effects models.**  Considering the bias of small sample size, we excluded the languages with below 25 participants in each data source before conducting the mixed-effects models. Thus we excluded Portuguese in the on-site data and Norwegian in the web-based data. Because the sources of data collection included the labs and the web, we had to evaluate whether one mixed-effects model sufficiently fitted all the data. Otherwise, separate models would be needed for each data set. This analysis showed a significant difference between data sources: `r paste0("*b* = ", source_cor_lmer_out["Estimate"], ", *SE* = ",source_cor_lmer_out["Std. Error"],", t( ",source_cor_lmer_out["df"]," ) = ",source_cor_lmer_out["t value"], ", *p* < .001")`. Thus, the on-site and the web-based data had to be analyzed separately. 

The final models examined the interaction between language and match advantage in each data source, as reported below. All other models are reported in Appendix 4. It must be acknowledged that the languages with larger sample sizes (see Tables \@ref(tab:summary-site) and \@ref(tab:summary-osweb)) have more reliable results. Furthermore, most of the languages were underpowered, being far from the 1,200 participants suggested by an a priori power analysis. 

In each data source, we compared the fit of the models with and without the random slope of matching condition. Both indicated that the models without the random slope had the best fit. The model from the on-site data revealed no significant effect of match advantage: `r paste0("*b* = ", site_cor_lme_out["Estimate"], ", *SE* = ",site_cor_lme_out["Std. Error"],", t( ",site_cor_lme_out["df"]," ) = ",site_cor_lme_out["t value"], ", *p* = ",site_cor_lme_out["Pr(>|t|)"])`. The model from the web-based data also failed to reveal a significant effect: `r paste0("*b* = ", osweb_cor_lme_out["Estimate"], ", *SE* = ",osweb_cor_lme_out["Std. Error"],", t( ",osweb_cor_lme_out["df"]," ) = ",osweb_cor_lme_out["t value"], ", *p* = ",osweb_cor_lme_out["Pr(>|t|)"])`. The latter model had a negative coefficient, unlike the on-site data. Although neither effect was significant, the difference in direction resounds with the match advantages and disadvantages found in experiments using the property of color [cf. @connellRepresentingObjectColour2007; @zwaanRevisitingMentalSimulation2012].


```{r plot-SP-site-lme, message=FALSE, warning=FALSE, fig.cap="Response times and standard error in the sentence-picture verification task by match condition in each language (on-site data only)."}
# Plot interaction
sjPlot::set_theme(axis.angle.x = 45,
                  axis.textsize = .8)

sjPlot::plot_model(site_cor.lmer, 
                   type = "eff", 
                   terms = c('Language', 'Match'), 
                #   ci.lvl = .95,
                   se = TRUE,
                    colors = "gs") +
  ylab("Response Time(ms)") + 
  labs(title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))

```

```{r plot-SP-osweb-lme, message=FALSE, warning=FALSE, fig.cap="Response times and standard error in the sentence-picture verification task by match condition in each language (web-based data only)."}
# Plot interaction
sjPlot::set_theme(axis.angle.x = 45,
                  axis.textsize = .8)

sjPlot::plot_model(osweb_cor.lmer, 
                   type = "eff", 
                   terms = c('Language', 'Match'), 
                #   ci.lvl = .95,
                   se = TRUE,
                    colors = "gs") +
  ylab("Response Time(ms)") + 
  labs(title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))

```


Figure \@ref(fig:plot-SP-site-lme) illustrates the response times from the on-site data. `r firstup(xfun::numbers_to_words(sum(summary(site_cor.lmer)$coefficients[2:14,"Pr(>|t|)"] < .05)))` languages presented significant intercepts (see “Models including languages” section in Appendix 4).   

(Insert Figure \@ref(fig:plot-SP-site-lme) about here)


Figure \@ref(fig:plot-SP-osweb-lme) illustrates the response times in the web-based data. `r firstup(xfun::numbers_to_words(sum(summary(osweb_cor.lmer)$coefficients[2:14,"Pr(>|t|)"] < .05)))` languages presented significant effects (see “Models included languages” section in Appendix 4).

(Insert Figure \@ref(fig:plot-SP-osweb-lme) about here)


**Anecdotal evidence on the match advantage.** In the on-site data, only Greek presented a match advantage, `r paste0("*b* = ", site_cor_lme_greek_out["Estimate"], ", *SE* = ",site_cor_lme_greek_out["Std. Error"],", t( ",site_cor_lme_greek_out["df"]," ) = ",site_cor_lme_greek_out["t value"], ", *p* = ",format(round(site_cor_lme_greek_out["Pr(>|t|)"],3),nsmall=3) )`. It should be noted, however, that these results are not robust due to the underpowered sample sizes (see Discussion).

The mean response times in Greek (*M* = `r round(mean(subset(SP_V_site_lme_data, Language=="Greek") %>% pull(response_time)),2)`, *SD* = `r round(sd(subset(SP_V_site_lme_data, Language=="Greek") %>% pull(response_time)),2)`) and Serbian (*M* = `r round(mean(subset(SP_V_osweb_lme_data, Language=="Serbian") %>% pull(response_time)),2)`, *SD* = `r round(sd(subset(SP_V_osweb_lme_data, Language=="Serbian") %>% pull(response_time)),2)`) was longer than the average across languages (M = `r round(mean(SP_V_lme_data$response_time),2)`, SD = `r round(sd(SP_V_lme_data$response_time),2)`). This might not be coincidental, as according to @yapRespondingNonwordsLexical2014, longer response times have been associated with larger effects in psycholinguistics [@schillingComparingNamingLexical1998; @seidenbergTimeCoursePhonological1985; @tainturierEducationalLevelWord1992].


```{r PP_data_preparation, message=FALSE, warning=FALSE}
## Dataset for mixed-effect model
PP_lme_data <- PP_tidy %>% 
  filter(Outlier == FALSE) %>% #filter outlier by time rule established above
  # consider excluding people who couldn't get these right 
  # using the same accuracy levels as above 
  left_join(rbind(PP_subj_osweb, PP_subj_site), 
             by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(P_Acc >= .70)
```


```{r PP_lang_lme, message=FALSE, warning=FALSE}

PP_lme_data <- mutate(PP_lme_data,
                        Identical= factor(Identical,
                          levels = c("Y","N"),
                          labels = c("SAME","DIFF")))

PP.lang.zero_slope.cor.lme <- lmerTest::lmer(response_time ~ 
                    Identical*Language +                # Fixed effect
                    (1 | Subject) +   # By-subject random intercept
                    (1 | Picture1) +   # By-item random intercept
                    (1 | PSA_ID),    # By-lab random intercept
                    data = PP_lme_data,
                    control = lmerControl(optimizer = "bobyqa",optCtrl = list(maxfun = 1e6)) # Increase maximum number of iterations to facilitate model convergence 
                    ) 
pp_cor_lme_out <- round(summary(PP.lang.zero_slope.cor.lme)$coefficients["IdenticalDIFF",],3)

#reported_p <- ifelse(pp_cor_lme_out["Pr(>|t|)"] < .001,"< .001", paste0("= ", pp_cor_lme_out["Pr(>|t|)"]))

```

```{r PP-sensitive, message=FALSE, warning=FALSE, include=FALSE}
coef_PP <- as.data.frame(summary(PP.lang.zero_slope.cor.lme)$coefficients)
coef_PP$sig_t <- qt(.025, coef_PP$df, lower.tail = F)
coef_PP$min_effect <- coef_PP$sig_t * coef_PP$`Std. Error`

mean(coef_PP[20:36 , "min_effect"])
```

**Analysis of imagery scores.** Prior to data collection, we assumed the imagery scores of every language group would be nearly equal. The best-fitting model included random intercepts for participants, targets and laboratories but no slopes for orientation. The fixed effect of orientation match was significant, `r paste0("*b* = ", pp_cor_lme_out["Estimate"], ", *SE* = ",pp_cor_lme_out["Std. Error"],", t( ",pp_cor_lme_out["df"]," ) = ",pp_cor_lme_out["t value"], ", *p* ", ifelse(pp_cor_lme_out["Pr(>|t|)"] < .001,"< .001", paste0("= ", pp_cor_lme_out["Pr(>|t|)"])))`. The response times illustrated in Figure \@ref(fig:plot-PP-lme) indicated that the imagery scores measured for each language were consistently positive supporting our hypothesis. The coefficients of all evaluated mixed-effects models are reported in Appendix 5. 

(Insert Figure \@ref(fig:plot-PP-lme) about here) 


```{r plot-PP-lme, message=FALSE, warning=FALSE, fig.cap="Response times and standard error in the picture-picture verification task by match condition in each language (both on-site and web-based data)."}
# Plot interaction
sjPlot::set_theme(axis.angle.x = 45,
                  axis.textsize = .8)

sjPlot::plot_model(PP.lang.zero_slope.cor.lme, 
                   type = "eff", 
                   terms = c('Language', 'Identical'), 
                #   ci.lvl = .95,
                   se = TRUE,
                    colors = "gs") +
  ylab("Response Time(ms)") + 
  labs(title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black"))

```



```{r prediction_data, message=FALSE, warning=FALSE}
## Dataset for traditional ANOVA
PP_aov_data <- PP_tidy %>% 
  filter(Outlier == FALSE) %>% #filter outlier by time rule established above
  # consider excluding people who couldn't get these right 
  # using the same accuracy levels as above 
  left_join(rbind(PP_subj_osweb, PP_subj_site), 
             by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(P_Acc >= .70) %>% 
  group_by(Source, Language, PSA_ID, Subject, Identical) %>%
  summarise(subject_M = median(response_time), subject_ACC = mean(P_Acc))


## I will merge the SP_V and PP data by participants' MEDIAN response times
model_site_data <- (SP_V_tidy %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  filter(Language != site_excluded_lang) %>% 
  filter(Source == "site") %>% 
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time)) %>%
  pivot_wider(names_from = Match, values_from = c(subject_M)) %>%
  mutate(Effect = (N - Y) )) %>%
  left_join(
    (PP_aov_data %>% select(-subject_ACC) %>%
  pivot_wider(names_from = Identical, values_from = subject_M) %>%
  mutate(Imagery = (N - Y))), by = c("Language","Subject")
  )

  
model_osweb_data <- (SP_V_tidy %>% 
  left_join(rbind(SP_V_subj_site, SP_V_subj_osweb), 
            by = c("Subject" = "Subject", 
                   "Language" = "Language", 
                   "Match" = "Match",
                   "PSA_ID" = "PSA_ID")) %>% 
  filter(Outlier == FALSE) %>% 
  filter(!is.na(V_Acc)) %>% # remove people who were less than 70% so they don't match with join
  filter(Language != site_excluded_lang) %>% 
  filter(Source == "osweb") %>% 
  group_by(Language, Subject, Match) %>%
  summarise(subject_M = median(response_time)) %>%
  pivot_wider(names_from = Match, values_from = c(subject_M)) %>%
  mutate(Effect = (N - Y) )) %>%
  left_join(
    (PP_aov_data %>% select(-subject_ACC) %>%
  pivot_wider(names_from = Identical, values_from = subject_M) %>%
  mutate(Imagery = (N - Y))), by = c("Language","Subject")
  )
```

The above analyses suggested that data sources did not influence the imagery scores but did influence the match advantage. Therefore, we evaluated the fit of the model with languages and imagery scores and the model with languages only. Both models included match advantage as the dependent variable. If imagery scores predict match advantage, the model with languages and imagery scores should fit the data better than the model with languages only. Because the random slopes for items in the analyses of the match advantage were zero (see Appendix 5), the data for building the regression models were the aggregated data by participants.

```{r site_model, message=FALSE, warning=FALSE, include=FALSE}

site_lang_model1 <- lm(Effect ~ Language*Imagery, data=model_site_data)
site_lang_model0 <- lm(Effect ~ Language, data=site_lang_model1$model)
site_test <- anova(site_lang_model0, site_lang_model1)[2,c("F","Pr(>F)")]
site_lang_results <- apa_print(site_lang_model0)
```


```{r osweb_model, message=FALSE, warning=FALSE, include=FALSE}
osweb_lang_model1 <- lm(Effect ~ Language*Imagery, data=model_osweb_data)
osweb_lang_model0 <- lm(Effect ~ Language, data=osweb_lang_model1$model)
osweb_test <- anova(osweb_lang_model0, osweb_lang_model1)[2,c("Res.Df","Df","F","Pr(>F)")]
osweb_lang_results <- apa_print(osweb_lang_model1)
```

In the linear regression analysis, we decided the best fit model from the model with only predictor, language, and the model with two predictors, languages and imagery scores. Because the analysis of match advantage revealed the difference between data sources, we conducted the regression analysis by the data source respectively. In the analysis of the on-site data, the model with language and imagery scores had yet fit better than the model with language only, `r paste0("*F* = ", round(site_test["F"],3), ", *p* = ", round(site_test["Pr(>F)"],3) )`. In contrast, in the analysis of the web-based data, the model with language and imagery scores had a better fit than the model with language only, `r paste0("*F* (",osweb_test["Df"],",",osweb_test["Res.Df"],") = ",round(osweb_test["F"],3),", *p* =", format(round(site_test["Pr(>F)"],3),nsamll=3) )`. In the latter case, the effect of imagery scores was nonsignificant, `r osweb_lang_results$full_result$Imagery`. Appendix 5 summarized the coefficients of the models included in these analyses. 

## Exploratory analysis: language-specific match advantages


```{r EN_data, message=FALSE, warning=FALSE, include=FALSE}
## Import data file for discussion
SP_V_eng_tidy <- SP_V_lme_data %>% # basically you want this dataset, already filtered for outliers and accuracy people but only english rather than pulling a file that was not included in this processing / or the OSF folder
  filter(Language == "English")
```



```{r TC_data_define, message=FALSE, warning=FALSE, include=FALSE}
SP_V_TC_tidy <- SP_V_lme_data %>% # basically you want this dataset, already filtered for outliers and accuracy people but only english rather than pulling a file that was not included in this processing / or the OSF folder
  filter(Language == "Traditional Chinese")
```



Based on the policy to conduct the linguistic-specific mixed-effect models, we selected the English datasets (N = `r format(length(unique(SP_V_eng_tidy$Subject)), scientific=FALSE, big.mark = ",")`) and the Traditional Chinese datasets (N = `r format(length(unique(SP_V_TC_tidy$Subject)), scientific=FALSE, big.mark = ",")`). For both languages, we are interested in whether the data sources could inhibit the match advantage. Another topic of interest is if the match advantage changed with English dialects, namely American English and British English.


```{r Eng_effect01_lme, message=FALSE, warning=FALSE, include=FALSE}
#SP_V_eng_tidy$r_System = if_else(SP_V_eng_tidy$System == "US",1,0)
SP_V_eng_tidy$r_System <- if_else(
  grepl("PSA|USA", SP_V_eng_tidy$PSA_ID),
  1, 0)

SP_V_eng_tidy$r_Source = if_else(SP_V_eng_tidy$Source == "Site",1,0)

eng_no_slope.lmer = lmerTest::lmer(response_time ~ Match*r_System*r_Source + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_eng_tidy)
```

Using the data from `r format(length(unique(SP_V_eng_tidy$Subject)), scientific=FALSE, big.mark = ",")` English speaking participants, we ran a mixed-effects model for the English data containing orientation match condition, English dialects (American vs. British) and data sources (on-site vs. web-based) as fixed effects. Following @brauerLinearMixedeffectsModels2018, English dialects and data sources were numerically recoded. The best fitted model indicated that only data source (on-site vs. web-based) was significant, `r paste0("*b* = ", round(summary(eng_no_slope.lmer)$coefficients["r_Source","Estimate"],3), ", *SE* = ",round(summary(eng_no_slope.lmer)$coefficients["r_Source","Std. Error"],3),", t( ",round(summary(eng_no_slope.lmer)$coefficients["r_Source","df"],3)," ) = ",round(summary(eng_no_slope.lmer)$coefficients["r_Source","t value"],3), ", *p* < .001")`. Although the match advantage of orientation was nonsignificant, this exploratory analysis indicated the interaction of orientation match condition and English dialects: `r paste0("*b* = ", round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","Estimate"],3), ", *SE* = ",round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","Std. Error"],3),", t( ",round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","df"],3)," ) = ",round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","t value"],3), ", *p* = ",format(round(summary(eng_no_slope.lmer)$coefficients["MatchMISMATCHING:r_System","Pr(>|t|)"],3), nsmall=3) )`  (see the detailed report in Appendix 4). 

<!---
Figure \@ref(fig:Eng-interaction-plot) showed the match advantages happened in the Internet data but only for American English.

(Insert Figure \@ref(fig:Eng-interaction-plot) about here)
--->

```{r Eng-interaction-plot, eval=FALSE, fig.cap="Estimated", message=FALSE, warning=FALSE, include=FALSE}
# Plot interaction

sjPlot::plot_model(eng_no_slope.lmer, 
                   type = "eff", 
                   terms = c('Match', 'r_Source', 'r_System'), 
                   #ci.lvl = .95,
                   se = TRUE,
                   colors = "gs",
                   show.legend = FALSE) +
  ylab("Response Time(ms)") + 
  labs(title = "",axis.title = "") + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        strip.background = element_blank(),
        strip.text = element_blank(),
        axis.line = element_line(colour = "black"))
```




```{r TC_effect01_lme, message=FALSE, warning=FALSE, include=FALSE}
SP_V_TC_tidy$r_Source = if_else(SP_V_TC_tidy$Source == "Site",1,0)

##TC_no_slope.lmer =lmer(response_time ~ Match*r_Source + (1|Subject) + (1|Target) + (1|PSA_ID), data = SP_V_TC_tidy)


##TC_cor.lmer =lmer(response_time ~ Match*r_Source + (1|Subject) + (1|Target) + (r_Source|PSA_ID), data = SP_V_TC_tidy)

##anova(TC_no_slope.lmer,TC_cor.lmer)    ## better fit: TC_no_slope.lmer

TC_no_slope.lmer = lmerTest::lmer(response_time ~ Match*r_Source + (1|Subject) + (1|Target) + (1|PSA_ID), control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e6)),  data = SP_V_TC_tidy)



```


We conducted another exploratory mixed-effect model on Traditional Chinese data because his was the only language to show a significant result in the preregistered meta-analysis. The best fit model had orientation match condition and data sources as the fixed effects. This model indicated that data source was significant, `r paste0("*b* = ", round(summary(TC_no_slope.lmer)$coefficients["r_Source","Estimate"],3), ", *SE* = ",round(summary(TC_no_slope.lmer)$coefficients["r_Source","Std. Error"],3),", t( ",round(summary(TC_no_slope.lmer)$coefficients["r_Source","df"],3)," ) = ",round(summary(TC_no_slope.lmer)$coefficients["r_Source","t value"],3), ", *p* < .001")`. The match advantage of orientation was nearly significant: `r paste0("*b* = ", round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING","Estimate"],3), ", *SE* = ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING","Std. Error"],3),", t( ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING","df"],3)," ) = ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING","t value"],3), ", *p* = ",format(round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING","Pr(>|t|)"],3)),nsmall=3)`, but the interaction of match advantage and data sources was nonsignificant: `r paste0("*b* = ", round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING:r_Source","Estimate"],3), ", *SE* = ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING:r_Source","Std. Error"],3),", t( ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING:r_Source","df"],3)," ) = ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING:r_Source","t value"],3), ", *p* = ",round(summary(TC_no_slope.lmer)$coefficients["MatchMISMATCHING:r_Source","Pr(>|t|)"],3))` (see the detailed report in Appendix 4). This result suggested that Traditional Chinese study could have a robust estimation in the circumstance multiple teams conducted the study in terms of one the same protocol. Combined with the previous results of Traditional Chinese [@chenDoesObjectSize2020], future research on this language could explore any potential linguistic aspects that might result in the match advantage of object orientation and other properties. Although this study is unable to provide further advice, the advantage for the future Traditional Chinese studies would be a precise sample size justification on the participants and stimulus items.